[
  {
    "objectID": "schedule.html",
    "href": "schedule.html",
    "title": "Timetable and objectives",
    "section": "",
    "text": "In this workshop, we explore RNAseq data preprocessing and analyses for differential expression experiments. Our goal for this workshop is to provide you with foundational knowledge required to understand the basic RNAseq workflow and responsibly apply these methods to your own research.\nThese sessions will provide you with an opportunity to develop your skills and get exposure to key concepts of RNAseq, practice running nf-core workflows, and performing downstream analyses in RStudio. We have outlined the timetable and learning objectives for each session below:"
  },
  {
    "objectID": "schedule.html#lesson-plan",
    "href": "schedule.html#lesson-plan",
    "title": "Timetable and objectives",
    "section": "Lesson plan",
    "text": "Lesson plan\n\n\n\n\n\n\n\n\n\nSession\nLesson\nOutcomes\nTime (min)\n\n\n\n\n1\nSession 1 kick-off\nDiscuss session 1 learning objectives, set up CLI environment on Nimbus VMs\n10 mins\n\n\n\nIntroduction to the case study\nExplore case study design and objectives\n10 mins\n\n\n\nnf-core/rnaseq explained\nDiscuss RNAseq experimental workflow and nf-core/rnaseq features\n20 mins\n\n\n\nBreak\n‚òï\n10 mins\n\n\n\nRaw data QC\nPerform raw data quality control and explore outputs\n20 mins\n\n\n\nExecute the data preprocessing workflow\nExecute full RNAseq data preprocessing workflow and examine nf-core/rnaseq functionality\n30 mins\n\n\n\nRead alignment and quantification\nExplore outputs of alignment and quantification processes\n10 mins\n\n\n\nBreak\n‚òï\n20 mins\n\n\n\nWorkflow performance\nEvaluate nf-core/rnaseq run technical performance using resource logs and data processing effectiveness using QC metrics\n15 mins\n\n\n\nSession 1 wrap-up\nDiscuss outcomes of data pre-processing and session 2 objectives and activities\n15 mins\n\n\n\n\n\n\n\n\n2\nSession 2 kick-off\nDiscuss session 2 learning objectives, log in to Nimbus VMs\n15 mins\n\n\n\nSet up Rstudio on Nimbus\nSet up Rstudio on Nimbus instances and explore functionality\n15 mins\n\n\n\nBreak\n‚òï\n10 mins\n\n\n\nExploratory analysis\nPerform RNAseq count matrix exploratory analysis and evaluate data quality\n40 mins\n\n\n\nBreak\n‚òï\n10 mins\n\n\n\nDifferential expression analysis\nIdentify and visualise differentially expressed genes\n20 mins\n\n\n\nFunctional enrichment analysis\nPerform functional enrichment analysis using gene ontologies and pathways\n25 mins\n\n\n\nSummarising the experiment\nDiscuss results of data processing and analysis and experimental outcomes\n15 mins\n\n\n\nSession 2 wrap-up\nDiscuss workshop outcomes, Q&A session\n25 mins"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Introduction to RNAseq workshop: reads to differential gene expression",
    "section": "",
    "text": "This workshop series will introduce you to RNA sequencing data analysis for differential expression and functional enrichment on the command line. We will use the nf-core/rnaseq pipeline to process raw sequence reads to a gene count matrix and then perform differential expression and enrichment analyses with R/Rstudio."
  },
  {
    "objectID": "index.html#target-audience",
    "href": "index.html#target-audience",
    "title": "Introduction to RNAseq workshop: reads to differential gene expression",
    "section": "Target audience",
    "text": "Target audience\nThis workshop series is suitable for people who are familiar with working at command line interface and may be new to RNAseq for differential expression analysis. The course is beginner friendly and intended for those interested in using the command line interface for their analysis. It is also suitable for those who want to learn about and use nf-co.re workflows."
  },
  {
    "objectID": "index.html#prerequisites",
    "href": "index.html#prerequisites",
    "title": "Introduction to RNAseq workshop: reads to differential gene expression",
    "section": "Prerequisites",
    "text": "Prerequisites\n\nFamiliarity with Unix/Linux command line\nFamiliarity with R/RStudio\nIntroduction to RNAseq webinar (slides)"
  },
  {
    "objectID": "index.html#get-the-most-out-of-this-workshop",
    "href": "index.html#get-the-most-out-of-this-workshop",
    "title": "Introduction to RNAseq workshop: reads to differential gene expression",
    "section": "Get the most out of this workshop",
    "text": "Get the most out of this workshop\nThis workshop series is presented across 2 days and will consist of code-along and breakout room sessions. You will get the most out of the workshop if you:\n\nBrush up on your command line basics before Day 1\nBrush up on your R/Rstudio skills\nWatch our webinar introducing RNAseq experiments\nSet up your computer before Day 1\nUse the Slack channel to ask questions and engage in discussions during the workshop\nDo the provided exercises during the workshop"
  },
  {
    "objectID": "index.html#workshop-schedule",
    "href": "index.html#workshop-schedule",
    "title": "Introduction to RNAseq workshop: reads to differential gene expression",
    "section": "Workshop schedule",
    "text": "Workshop schedule\n\n\n\nLesson\nOverview\n\n\n\n\nSet up\nFollow these instructions to install a terminal application and web browser to be used in the workshop.\n\n\nDay 1\nParticipants will use nf-core‚Äôs rnaseq pipeline to transform raw sequencing data into analysis ready count data.\n\n\nDay 2\nParticipants will use RStudio to perform differential expression and enrichment analysis on count matrix."
  },
  {
    "objectID": "index.html#course-survey",
    "href": "index.html#course-survey",
    "title": "Introduction to RNAseq workshop: reads to differential gene expression",
    "section": "Course survey!",
    "text": "Course survey!\nPlease fill out our course survey before you leave! Help us help you! üòÉ"
  },
  {
    "objectID": "index.html#credits-and-acknowledgements",
    "href": "index.html#credits-and-acknowledgements",
    "title": "Introduction to RNAseq workshop: reads to differential gene expression",
    "section": "Credits and acknowledgements",
    "text": "Credits and acknowledgements\nLead trainers:\n\nDr Nandan Deshpande\nDr Georgie Samaha\n\nThis training was developed under the Australian BioCommons ‚ÄòBring Your Own Data‚Äô - Command Line Interface (CLI) Platform Expansion Project by the Sydney Informatics Hub, in partnership with Pawsey Supercomputing Centre and Australian Academic Research Network (AARNet). This project aims to create highly accessible, highly available, highly scalable analysis and data sharing capabilities for the benefit of life science researchers nationally."
  },
  {
    "objectID": "notebooks/2.4_enrichment.html",
    "href": "notebooks/2.4_enrichment.html",
    "title": "Functional enrichment analysis in R",
    "section": "",
    "text": "Learning objectives\n\n\n\n\nUnderstand and apply functional enrichment analysis in the context of RNAseq DE experiments.\nDefine gene ontologies.\nPerform functional enrichment analysis on a set of differentially expressed genes.\nWe now have a list of significant DE genes. To gain greater biological insights on the DE genes we can determine if there is enrichment of known biological functions, pathways or interactions. We are at the final stage of our RNA experimental workflow:\nThere are different methods for identifying functional enrichments, e.g.:\nEnrichment analysis can be done using some of the freely available web- and R-based tools. Today we will use a R-package clusterProfiler to perform Over representation analysis. You can read more about the GSEA method here\nIn this lesson, we will perform over representation analysis (ORA) using gene ontology databases."
  },
  {
    "objectID": "notebooks/2.4_enrichment.html#over-representation-analysis",
    "href": "notebooks/2.4_enrichment.html#over-representation-analysis",
    "title": "Functional enrichment analysis in R",
    "section": "2.4.1 Over-representation analysis",
    "text": "2.4.1 Over-representation analysis\nThere are many functional enrichment tools that perform over-representation analysis using curated databases that categorise genes into groups based on shared function, or involvement in a pathway, or presence in a specific cellular location. These categories are independent of any organism, however each organism has distinct categories available.\n\nTo determine whether any categories are over-represented, you can determine the probability of having the observed proportion of genes associated with a specific category in your gene list based on the proportion of genes associated with the same category in the background gene set (gene category for the appropriate organism)."
  },
  {
    "objectID": "notebooks/2.4_enrichment.html#gene-ontologies",
    "href": "notebooks/2.4_enrichment.html#gene-ontologies",
    "title": "Functional enrichment analysis in R",
    "section": "2.4.2 Gene Ontologies",
    "text": "2.4.2 Gene Ontologies\nOne of the most widely-used categorisation databases is the Gene Ontology (GO) resource. To describe the functional roles of genes and gene products, they are grouped into GO terms. GO terms are organised into three independent controlled vocabularies (ontologies) in a species-independent manner:\n\nBiological process: Refers to the biological role involving the gene or gene product, and could include ‚Äútranscription‚Äù, ‚Äúsignal transduction‚Äù, and ‚Äúapoptosis‚Äù. A biological process generally involves a chemical or physical change of the starting material or input.\nMolecular function: Represents the biochemical activity of the gene product, such activities could include ‚Äúligand‚Äù, ‚ÄúGTPase‚Äù, and ‚Äútransporter‚Äù.\nCellular component: Refers to the location in the cell of the gene product. Cellular components could include ‚Äúnucleus‚Äù, ‚Äúlysosome‚Äù, and ‚Äúplasma membrane‚Äù.\n\nGO enrichment analysis tools will determine GO terms that are enriched when you supply it will a list of DE genes."
  },
  {
    "objectID": "notebooks/2.4_enrichment.html#visualise-functional-profiles-of-genes",
    "href": "notebooks/2.4_enrichment.html#visualise-functional-profiles-of-genes",
    "title": "Functional enrichment analysis in R",
    "section": "2.4.3 Visualise functional profiles of genes",
    "text": "2.4.3 Visualise functional profiles of genes\nThe clusterProfiler R package implements methods to analyse and visualise functional profiles of genes. The clusterProfiler R-library supports functional characteristics of both coding and non-coding genomics data for thousands of species with up-to-date gene annotation. It provides a tidy interface to access, manipulate, and visualise enrichment results to help users achieve efficient data interpretation."
  },
  {
    "objectID": "notebooks/2.4_enrichment.html#prepare-de-genes-for-enrichment-analysis",
    "href": "notebooks/2.4_enrichment.html#prepare-de-genes-for-enrichment-analysis",
    "title": "Functional enrichment analysis in R",
    "section": "2.4.4 Prepare DE genes for enrichment analysis",
    "text": "2.4.4 Prepare DE genes for enrichment analysis\nWe will use clusterProfiler to perform enrichment analysis for upregulated and downregulated significant DE genes separately. We can use the resSig005_subset_lfc dataframe created earlier and prepare the data for clusterProfiler.\n# We will use the resSig005_subset_lfc dataframe created earlier\n# This has been filtered for padj < 0.05 and |LFC| > 1 DE genes\n\n# Upregulated genes\n# Filter for significant upregulated genes by log2 fold change > 1. Remove NAs.\nsig.up <- resSig005_subset_lfc[resSig005_subset_lfc$log2FoldChange > 1, ]\nsig.up <- na.omit(sig.up)\n# Create list by decreasing log fold change for upregulated genes\nsig.up.LFC <- sig.up$log2FoldChange\nnames(sig.up.LFC) <- rownames(sig.up)\n# Sort by LFC, decreasing\nsig.up.LFC <- sort(sig.up.LFC, decreasing = TRUE)\n\n# Downregulated genes - let's do the same thing\nsig.dn <- resSig005_subset_lfc[resSig005_subset_lfc$log2FoldChange < -1, ]\n# Filter for significant upregulated genes by log2 fold change < -1. Remove NAs.\nsig.dn <- na.omit(sig.dn)\n# Create list by decreasing log fold change for upregulated genes\nsig.dn.LFC <- sig.dn$log2FoldChange\nnames(sig.dn.LFC) <- rownames(sig.dn)\n# Sort by LFC, decreasing\nsig.dn.LFC <- sort(sig.dn.LFC, decreasing = TRUE)\nYou can check that you have correctly prepared the data by inspecting sig.up.LFC and sig.dn.LFC in the console. Use class() to check what type of R class we have stored the data in."
  },
  {
    "objectID": "notebooks/2.4_enrichment.html#go-enrichment-of-upregulated-genes",
    "href": "notebooks/2.4_enrichment.html#go-enrichment-of-upregulated-genes",
    "title": "Functional enrichment analysis in R",
    "section": "2.4.5 GO enrichment of upregulated genes",
    "text": "2.4.5 GO enrichment of upregulated genes\nClusterprofiler implements the function enrichGO() for gene ontology over-representation test. Let‚Äôs run the analysis for up-regulated genes. You can check what the different parameters in the function do by running the command ?enrichGO. The command will take a few minutes to run.\nego.up <- enrichGO(gene = names(sig.up.LFC),\n                   OrgDb = org.Mm.eg.db, \n                   keyType = 'SYMBOL',\n                   readable = FALSE,\n                   ont = \"ALL\",\n                   pAdjustMethod = \"BH\",\n                   pvalueCutoff = 0.05, \n                   qvalueCutoff = 0.05)\n\nBar plot of upregulated enriched GO terms\nThe bar plot is a commonly used method to visualise enriched terms. It depicts the enrichment scores (e.g.¬†p-adj values) and gene count or ratio as bar height and colour.\nbarplot(ego.up, \n        showCategory = 20)\n\n\nDot plot of upregulated enriched GO terms\nA dot plot is similar to a scatter plot and bar plot with the capability to encode another score as dot size. In R the dot plot displays the index (each category) in the vertical axis and the corresponding value in the horizontal axis, so you can see the value of each observation following a horizontal line from the label.\ndotplot(ego.up, \n        showCategory = 20,\n        font.size = 8)\n\n\ncnetplot of upregulated enriched GO terms\nBoth the barplot and dotplot only displayed most significant enriched terms, while users may want to know which genes are involved in these significant terms. The cnetplot depicts the linkages of genes and biological concepts (e.g.¬†GO terms or KEGG pathways) as a network.\ncnetplot(ego.up, \n         categorySize = \"pvalue\", \n         foldChange = sig.up.LFC,\n         cex_label_gene = 0.7,\n         showCategory = 5,\n         cex_label_category = 1.2,\n         shadowtext = 'category')\n\n\nHeatmap-like functional classification of upregulated enriched GO terms\nThe heatplot is similar to cnetplot, while displaying the relationships as a heatmap. The gene-concept network may become too complicated if user want to show a large number significant terms. The heatplot can simplify the result and more easy to identify expression patterns.\nheatplot(ego.up,\n         showCategory = 20,\n         foldChange = sig.up.LFC)\n\n\n\n\n\n\nChallenge exercise\n\n\n\nTry performing the above steps for downregulated genes. Please remember to rename the variable differently so as to not over-write them e.g.¬†ego.dn in place of ego.up\nThis will be a good time to familiarise yourself with the various functions you have used. Remember, you can try and run ?Function_NAME e.g.¬†?enrichGO to get the help manual pages for that function. You can play with pvalue and qvalue cutoffs, categories etc. Try and interpret the results. We will discuss them at the end of this session.\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nego.dn <- enrichGO(gene = names(sig.dn.LFC),\n                   OrgDb = org.Mm.eg.db, \n                   keyType = 'SYMBOL',\n                   readable = FALSE,\n                   ont = \"ALL\",\n                   pAdjustMethod = \"BH\",\n                   pvalueCutoff = 0.05, \n                   qvalueCutoff = 0.05??q)\nBar plot of downregulated enriched GO terms\nbarplot(ego.dn,\n        showCategory = 20)\nDot-plot of downregulated enriched GO terms\ndotplot(ego.dn, \n        showCategory = 20,\n        font.size = 10)\ncnetplot of downregulated enriched GO terms\ncnetplot(ego.dn,\n         categorySize = \"pvalue\", \n         foldChange = sig.dn.LFC,\n         cex_label_gene = 0.7,\n         showCategory = 5,\n         cex_label_category = 1.5,\n         shadowtext = 'category')\nHeatmap-like functional classification of downregulated enriched GO terms\nheatplot(ego.dn,\n         showCategory = 20,\n         foldChange = sig.dn.LFC)"
  },
  {
    "objectID": "notebooks/2.4_enrichment.html#other-functional-enrichment-tools",
    "href": "notebooks/2.4_enrichment.html#other-functional-enrichment-tools",
    "title": "Functional enrichment analysis in R",
    "section": "2.4.6 Other functional enrichment tools",
    "text": "2.4.6 Other functional enrichment tools\n\nOther enrich functions in clusterprofiler\nclustrprofiler has other functions\n\nenrichKEGG KEGG\nenrichWP WikiPathways\nenrichPathway Reactome\nenrichr Enrichr resource\n\n\n\nOpen source tools\n\nPantherdb http://pantherdb.org/\nEnrichr https://maayanlab.cloud/Enrichr/\nDavid https://david.ncifcrf.gov/\ngprofiler https://biit.cs.ut.ee/gprofiler/ And many more ‚Ä¶\n\n\n\nCommercial tools\n\nIngenuity Pathway Analysis (IPA) https://www.qiagen.com/us/\nMetacore https://portal.genego.com/"
  },
  {
    "objectID": "notebooks/2.4_enrichment.html#sessioninfo",
    "href": "notebooks/2.4_enrichment.html#sessioninfo",
    "title": "Functional enrichment analysis in R",
    "section": "2.4.7 sessionInfo()",
    "text": "2.4.7 sessionInfo()\nIt is good practice to record the version of R and all tools you are using for reproducibility purposes (and for the methods section in your paper!). R‚Äôs sessionInfo() prints all of this information.\nsessionInfo()\n\nKey points\n\nFunctional enrichments can guide our intepretation of results in their biological context.\nFunctional enrichments can include pathways and gene ontologies"
  },
  {
    "objectID": "notebooks/2.2_exploratory.html",
    "href": "notebooks/2.2_exploratory.html",
    "title": "Exploratory analysis in R",
    "section": "",
    "text": "Learning objectives\n\n\n\n\nPerform exploratory analysis of a count matrix using DESeq2\nPerform principal component analysis using gene counts to capture sample distribution\nEvaluate the quality of the data and experimental design\nBefore performing any DE analysis, it is good to explore and visualise our data. This helps us get a sense of the quality of our data at this stage of the analysis. We‚Äôll be performing the following techniques in order to evaluate the quality of our raw counts data. We have proceeded to the next step of our RNAseq experimental workflow and will be preparing our data for differential expression analysis in this lesson:"
  },
  {
    "objectID": "notebooks/2.2_exploratory.html#examine-the-total-library-size",
    "href": "notebooks/2.2_exploratory.html#examine-the-total-library-size",
    "title": "Exploratory analysis in R",
    "section": "2.2.1 Examine the total library size",
    "text": "2.2.1 Examine the total library size\nFor each sample, check the total library size. This is essentially the total number of reads which we have aligned to each sample.\n# Sum raw gene counts for each sample (each column)\ncolSums(counttable)\n\n\n\n\n\n\nReflection exercise\n\n\n\nHow do you think the differences in total library size could affect differential expression analysis?\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nNotice that the counts are different for each sample, which we will need to account for. Total library size can give us an indication of sensitivity (more reads = higher ability to detect lowly expressed genes)."
  },
  {
    "objectID": "notebooks/2.2_exploratory.html#examine-raw-data-distribution",
    "href": "notebooks/2.2_exploratory.html#examine-raw-data-distribution",
    "title": "Exploratory analysis in R",
    "section": "2.2.2 Examine raw data distribution",
    "text": "2.2.2 Examine raw data distribution\nHere we plot a boxplot of gene level raw counts (y axis) for each sample (x axis).\nboxplot(counttable,\n        col=\"red\")\n\n\n\n\n\n\nReflection exercise\n\n\n\nHow would you describe what you are seeing in the plot?\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nFor most genes, the total number of raw counts for each gene are relatively low. We can visualise distribution better if the counts were on a log2 scale.\n\n\n\n# Add 1 to make sure all values are > 0\nboxplot(log2((counttable)+1),\n        col = \"red\")\nThe distribution of counts across samples is not comparable (although not too dissimilar in this case!). This is a consideration to take if you plan to use any statistical tests that assume an equal distribution of counts across samples."
  },
  {
    "objectID": "notebooks/2.2_exploratory.html#exploratory-analysis-with-deseq2",
    "href": "notebooks/2.2_exploratory.html#exploratory-analysis-with-deseq2",
    "title": "Exploratory analysis in R",
    "section": "2.2.3 Exploratory analysis with DESeq2",
    "text": "2.2.3 Exploratory analysis with DESeq2\nThe DESeq2 package contains functions that perform normalisation, data transformation, visualisation and DE. This is a highly regarded and popular tool. We will use some of its functions to perform exploratory analysis.\nThis is a large package and we will only scratch the surface of key concepts in this workshop. We recommend you read the DESeq2 paper and manual before performing your own analysis.\n\nThe original DESeq 2 paper, Love et al.¬†2014\nThe DESeq2 manual\nA beginners guide (written by the DESeq2 authors)"
  },
  {
    "objectID": "notebooks/2.2_exploratory.html#experimental-design-and-the-deseqdataset-object",
    "href": "notebooks/2.2_exploratory.html#experimental-design-and-the-deseqdataset-object",
    "title": "Exploratory analysis in R",
    "section": "2.2.4 Experimental design and the DESeqDataSet object",
    "text": "2.2.4 Experimental design and the DESeqDataSet object\nIn order for DESeq2 to perform DE, we need to store data in a DESeqDataSet object (dds) which contains:\n\nOur count matrix file\nOur experimental information (‚Äúmeta‚Äù) file\nOur design formula\n\nFor exploratory analysis, we set design = ~ 1 which tells DESeq2 to be blind to experimental groups. We do not want DESeq2 to account for any within group variability during exploratory analysis and quality checking. This will allow us to observe for any unexpected batch effects.\nWe will spend more time understanding the dds object later in this workshop.\n# We will call this object by name 'dds' as this is a standard practice\ndds <- DESeqDataSetFromMatrix(countData = counttable, \n                              colData = meta, \n                              design = ~1)\n\n2.2.5 Data transformation\nCount data is transformed with regularized log (rlog) or variance stabilising transformation (vst), required before performing exploratory data analysis such as visualisation and clustering (e.g.¬†PCA). Both methods produce data on the log2 scale, and normalize for other factors such as library size.\nrlog performs slightly better, but can take a lot longer than vst if you have many samples. We will set blind = TRUE so that DESeq2 is blind to experimental groups, for the same reasons as previously described.\n# Calculate rlog and store it in a dds-like object\nrlog <- rlog(dds, blind = TRUE)\nrlog.data <- assay(rlog)\nWe can check the effect of transformation by again using boxplots.\nboxplot(rlog.data,\n        col = \"red\")\nNotice that the count distribution across samples is much more comparable with rlog transformed data."
  },
  {
    "objectID": "notebooks/2.2_exploratory.html#principal-component-analysis",
    "href": "notebooks/2.2_exploratory.html#principal-component-analysis",
    "title": "Exploratory analysis in R",
    "section": "2.2.6 Principal component analysis",
    "text": "2.2.6 Principal component analysis\nPrincipal component analysis (PCA) is a dimensionality reduction method that summarises high dimensionality data into a number of principal components (PCs). For RNA-Seq, our highly dimensional data is our per sample gene-expression data and the variance that exists across samples. We can observe the relationship between these in a 2D space, by plotting two components at a time (usually the top two that account for most of the variance).\nCreate a PCA plot using rlog transformed data. By default, DESeq2::plotPCA() will colour each dot by the sample‚Äôs experimental group, but we have included some additional code to remove these for our discussion.\n# DESeq2's plotPCA function will create a PCA plot using an object that has rlog or vst values\npcaData <- plotPCA(rlog, returnData=TRUE)\n\n# Convert to percentages\npercentVar <- round(100 * attr(pcaData, \"percentVar\"))\n\n# Plot table\nggplot(pcaData, aes(PC1, PC2)) +\n  geom_point(size=3) +\n  xlab(paste0(\"PC1: \",percentVar[1],\"% variance\")) +\n  ylab(paste0(\"PC2: \",percentVar[2],\"% variance\")) + \n  coord_fixed()\n\n\n\n\n\n\nReflection exercise\n\n\n\nEach dot represents one sample. Samples that appear closer together have a more similar gene expression profile. Can you speculate which samples belong to the same experimental group?\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThe 3 on the left are likely to belong to one experimental group, and the 3 on the right to another.\n\n\n\nLet‚Äôs recreate the plot, now colouring samples by their experimental group.\n# DESeq2's plotPCA function will create a PCA plot using an object that has rlog or vst values\nDESeq2::plotPCA(rlog)\n\n\n\n\n\n\nReflection exercise\n\n\n\n\nCan you comment on how the samples cluster together in the plot?\nIf you saw one red dot cluster more closely with the blue dots, what might this suggest?\nApart from experimental groups, what other relationships might be revealed when looking at PCA plots?\nHow much of the overall variance is explained by PC1 & PC2?\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nThe wild samples cluster together nicely, as do the knockout, although the knockout samples are a little bit more spread out.\nThe gene expression profile of that knockout is more similar to the wildtype mice and the knockout might not have worked. This is not the case here and we have checked our alignments - all knockouts do not have exon 1 of the Gtf2ird1 gene.\nBatch effects can be very tricky to deal with and ideally, you would implement strategies to avoid or control for unwanted batch effects before starting your experiment.\n90%. As PC1 and PC2 explain most of the variance observed in the dataset, we do not check the other PCs.\n\n\n\n\nPlotting the other PCs is something you may want to do until you have explored most of the variation in the dataset and what their potential sources might be. We will not have time to cover this in the workshop, but do recommend you look into scree plots and observing the genes contributing to each PC."
  },
  {
    "objectID": "notebooks/2.2_exploratory.html#sample-to-sample-distances-heatmap",
    "href": "notebooks/2.2_exploratory.html#sample-to-sample-distances-heatmap",
    "title": "Exploratory analysis in R",
    "section": "2.2.7 Sample-to-sample distances heatmap",
    "text": "2.2.7 Sample-to-sample distances heatmap\nAnother way to visualise how similar or dissimilar our samples are is to plot sample distances in a heatmap and a hierarchical cluster.\n# dist() calculates eucleadean distance, which requires data to be in a specific format\nsampleDists <- dist(t(assay(rlog)))\nsampleDistMatrix <- as.matrix(sampleDists)\n\n# Get some pretty blue colours\ncolors <- colorRampPalette( rev(brewer.pal(9, \"Blues\")) )(255)\n\n# Plot the sampleDistMatrix in a heatmap\n# pheatmap also calculates and plots hierachical clusters \npheatmap(sampleDistMatrix,\n         clustering_distance_rows = sampleDists,\n         clustering_distance_cols = sampleDists,\n         col = colors)\nDark blue indicates samples that are more similar (distance = 0).\n\n\n\n\n\n\nReflection exercise\n\n\n\nWhat do you notice about sample KO3?\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThe hierarchical cluster groups KO3 more closely with the wild type samples. This could be an indication that our knockout has not worked - although we have confirmed that it has in part 1 of this workshop and are hence not too worried about it. Another reason could be that the knockout effect is not as strong in this sample as the other KO samples - for reasons we do not yet know!\n\n\n\n\n\n\n\n\n\nKey takeaways\n\n\n\n\nPerforming exploratory analysis is required in order to understand our data and decide if the experimental variability is explained by the conditions of interest.\nExploratory analysis can also help to identify and avoid possible outlier samples from the experiment."
  },
  {
    "objectID": "notebooks/1.0_casestudy.html",
    "href": "notebooks/1.0_casestudy.html",
    "title": "Intro to the case study",
    "section": "",
    "text": "Learning objectives\n\n\n\n\nConsider experimental design considerations for RNAseq differential expression analysis\nIn this workshop, we are working with a real dataset from a knockout mouse model study by Corley et al.¬†(2016). This study used the mouse model to study Williams-Beuren Syndrome (WBS), a rare genetic disease in people.\nWilliams-Beuren Syndrome affects multiple body systems and has physical, cognitive, and behavioural traits. Previous studies of WBS patients with deletions of the 7q11.23 chromosomal region have led to the conclusion that a disruption of Gtf2ird1 and Gtf2i genes explain the features of WBS, including:\nCorley et al.¬†(2016) used lip tissue from a Gtf2ird1 knockout mouse model to capture phenotypic effects most present in the epidermal tissues. Gtf2ird1 is located on chromosome 5 (5qG2) of the mouse genome."
  },
  {
    "objectID": "notebooks/1.0_casestudy.html#the-study-design",
    "href": "notebooks/1.0_casestudy.html#the-study-design",
    "title": "Intro to the case study",
    "section": "The study design",
    "text": "The study design\n\n6 mice\n\n3 knockouts (missing exon 1 of Gtf2ird1)\n3 wildtypes (not missing exon 1 of Gtf2ird1)\n\nmRNA extracted from epidermal (lip) tissue\n101 bp paired-end reads sequenced on Illumina HiSeq2000\n\n\n\n\n\n\n\nWhat is a knock out mouse model?\n\n\n\nCreating a knockout (KO) mouse model involves genetically engineering a mouse strain in which researchers have selectively and specifically disrupted (knocked out) the function of a particular gene. These models are useful for understanding the role of specific genes in health and disease."
  },
  {
    "objectID": "notebooks/1.0_casestudy.html#research-questions",
    "href": "notebooks/1.0_casestudy.html#research-questions",
    "title": "Intro to the case study",
    "section": "Research questions",
    "text": "Research questions\n\nWhich genes (if any) are upregulated or downregulated in our knockout mice compared with normal mice?\nHow do these differentially expressed genes relate to the disease phenotype?‚Äã\n\n\n\n\n\n\n\nReflection exercise\n\n\n\n\nWhich sorts of genes do you expect will be up/downregulated in KO compared with WT?\nCan you identify a potential limiting factor in the experimental design which might affect result interpretation?\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nCompensatory mechanisms countering the effect of the KO. Downregulation of genes regulated by Gtf2ird1\nSmall sample size lacking statistical power. Lip epidermal tissue fails to represent changes in other tissue types (neuro). KO of single exon might retain partial functionality.\n\n\n\n\nAs you work through the materials presented here, you‚Äôll need to recall experimental design choices made by Corley et al.¬†(2016) and research aims. We will revisit this page numerous times throughout this workshop.\n\n\n\n\n\n\nKey takeaways\n\n\n\n\nCorley et al.¬†(2016) used a knockout mouse model to model Williams-Beuren Syndrome (WBS), a rare genetic disease in people.\n\nOur research questions center on understanding gene regulation in knockout mice relative to normal mice and how these changes in gene expression might relate to the disease phenotype.\nPossible limitations in study design and sample collection may limit our ability to answer all research questions posed by this study."
  },
  {
    "objectID": "notebooks/1.5_outputs.html",
    "href": "notebooks/1.5_outputs.html",
    "title": "Evaluate data pre-processing pipeline performance",
    "section": "",
    "text": "Learning objectives\n\n\n\n\nLearn how to interpret an aggregated MultiQC report\nLearn how to interpret Nextflow workflow execution reports\nA number of summary reports are generated by the nf-core/rnaseq pipeline. These include:\nThese reports can be useful in evaluating the performance of your workflow execution, both to ensure your methods are accurate and that you are using resources efficiently."
  },
  {
    "objectID": "notebooks/1.5_outputs.html#workflow-quality-control",
    "href": "notebooks/1.5_outputs.html#workflow-quality-control",
    "title": "Evaluate data pre-processing pipeline performance",
    "section": "1.5.1 Workflow quality control",
    "text": "1.5.1 Workflow quality control\nAs we process any bioinformatics datasets using a workflow or pipeline, it is important that we document the parameters we have used to execute the workflow and the results at each step. This will ensure our analysis is reproducible by our future selves and others. By tracking the results output by each stage of the workflow, we can track potential issues with out data or the parameters we are using as well as identify things like contamination or biases that may impede our downstream analyses.\nThe nf-core/rnaseq pipeline generates comprehensive QC stats for all steps. These metrics are all aggregated into a single report using MultiQC. MultiQC is a popular tool for aggregating quality control metrics output by a number of popular bioinformatics tools. It generates interactive reports and outputs them as .html files that can be viewed with a web browser.\n‚û§ We have previously looked through MultiQC report in this session, but lets take a closer look in this lesson. Open the MultiQC report created at the end of our post-QC pipeline execution.\n/home/training/Day-1/WBS-mouse-results/multiqc/star_salmon/multiqc_report.html\nOn the left hand side, you will see a number of sections and subsections. The nf-core/rnaseq pipeline runs lots of different tools throughout the workflow to identify possible issues that may affect downstream analysis and summarise the results of each process your sequence data is run through. These are all aggregated here.\nLooking at the table at the top under General Stats, we can see a comparison of various QC metrics across all samples. From this all our samples appear to have consistent high-quality results:\n\n\n\n\n\n\n\nReflection exercise\n\n\n\nPlease review the MultiQC report and answer the following questions:\n\nWhat sort of variability, if any, do you observe amongst our samples?\nWhere is the nf-core/rnaseq execution command we ran presented in this report?\nWhat version of STAR did we run?\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nSamples varied in number of sequence reads, with SRR3473988 having significantly more reads than all other samples. There was also some variability in the % of reads uniquely aligned for each sample by STAR. Overall, sample performance was comparable and of high quality.\nnf-core/rnaseq Methods Description\n2.7.9a"
  },
  {
    "objectID": "notebooks/1.5_outputs.html#workflow-execution-performance",
    "href": "notebooks/1.5_outputs.html#workflow-execution-performance",
    "title": "Evaluate data pre-processing pipeline performance",
    "section": "1.5.2 Workflow execution performance",
    "text": "1.5.2 Workflow execution performance\nNextflow has some inbuilt reporting features that are used by the nf-core/rnaseq pipeline. The nf-core/rnaseq pipeline outputs these reports to the directory we ran the pipeline from (/home/training/Day-1/WBS-mouse-results/pipeline_info). The information presented in these reports helps to get an estimate of the required resources for independent processes and assign them for future runs.\n‚û§ nf-core pipelines generate pipeline summary reports by default. View all the outputs in the pipeline_info directory:\nls -lh pipeline_info\ntotal 3716\n-rw-rw-r-- 1 training training 3383766 Sep 29 03:55 execution_report_2023-09-29_03-40-43.html\n-rw-rw-r-- 1 training training  302879 Sep 29 03:55 execution_timeline_2023-09-29_03-40-43.html\n-rw-rw-r-- 1 training training   29901 Sep 29 03:55 execution_trace_2023-09-29_03-40-43.txt\n-rw-rw-r-- 1 training training   65615 Sep 29 03:55 pipeline_dag_2023-09-29_03-40-43.html\n-rw-rw-r-- 1 training training     953 Sep 29 03:40 samplesheet.valid.csv\n-rw-rw-r-- 1 training training    1478 Sep 29 03:55 software_versions.yml\n\nExecution report\nNextflow execution reports are broken down into:\n\nSummary: execution status, run command, execution time, other workflow metadata.\nResources: plots of disturbution of CPU, RAM usage for each workflow process.\nTasks: all tasks executed by the run command, their status, and command run.\n\nAs an example, looking at the job duration plot below, it appears STAR was the longest running process for all samples. From the boxplot, we can also see there was quite a bit of variation in runtimes for STAR between samples.\n\n\n\nExecution timeline\nNextflow execution timeline files contain bar plots for each process, how long they took to run and the maximum amount of memory consumed. If we look at the STAR task runtimes for each sample in the plot below, we can confirm that the time taken to align reads for each sample ranged from 35 seconds to over a minute. These tasks all also consumed significantly more RAM, than all other processes.\n\n\n\n\n\n\n\nKey takeaways\n\n\n\n\nNextflow provides easy to understand html log files, which can be used to understand the system resource requirements.\nEnsuring the reproducibility and accuracy of RNAseq data analyses is crucial and QC metrics should be collected to ensure the success of each step.\nThe nf-core/rnaseq pipeline uses MultiQC to aggregate summary and quality metrics for each step of the pipeline execution."
  },
  {
    "objectID": "notebooks/1.3_nfcore-run.html",
    "href": "notebooks/1.3_nfcore-run.html",
    "title": "Data pre-processing with nf-core/rnaseq",
    "section": "",
    "text": "Learning objectives\n\n\n\n\nUnderstand how to apply parameters to an nf-core pipeline to cusomise its execution\nUnderstand the nf-core workflow execution standard output\nNow that we have confirmed our data is of good quality and trimmed low quality bases and adapter sequences from our reads, we can proceed with running the remainder of the nf-core/rnaseq pipeline to complete the read alignment and quantification steps (red box below).\n."
  },
  {
    "objectID": "notebooks/1.3_nfcore-run.html#run-the-data-preprocessing-workflow",
    "href": "notebooks/1.3_nfcore-run.html#run-the-data-preprocessing-workflow",
    "title": "Data pre-processing with nf-core/rnaseq",
    "section": "1.3.1 Run the data preprocessing workflow",
    "text": "1.3.1 Run the data preprocessing workflow\nA nice feature of Nextflow and nf-core pipelines is the ability to resume an incomplete workflow execution from the last successfully completed step. We will use this feature to save ourselves time and pick up from after read trimming.\n\n\n\n\n\n\nNextflow resume function explained\n\n\n\nOne of Nextflow‚Äôs key features is the ability to resume a workflow from the point of failure, rather than starting over from scratch. When running any Nextflow workflow, you can use the -resume flag to use this feature.\nNextflow uses a caching system to track the results of each task in a workflow. This cache will include both the input files and output files created for each task. If your workflow execution is interrupted for any reason, some tasks may have completed while others have not. Using -resume tells Nextflow to check the cache to see which tasks have already finishe. Notice the appearance of a work directory when you run your Nextflow pipelines. For tasks that were not completed or quit with an error, it will attempt to rerun them.\n\n\n‚û§ In your terminal, check where you are by running:\npwd\n/home/training/Day-1\nIf you are not in the Day-1 directory, move into it by running:\ncd /home/training/Day-1 \nWe are going to adjust our run command slightly from last time, note:\n\nThe -resume flag at the end of the command\nWe will output our results in a different directory (--outdir)\nWe have added an extra flag to skip duplicate marking (--skip_markduplicates)\n\nNote that we have not specified a read alignment or quantification tool. We will be running the default setting which runs STAR to map the raw fastq reads to the reference genome and perform the read quantification using the alignment files with Salmon. You can specify which alignment method you‚Äôd like to use with --aligner.\n\n\n\n\n\n\nA note on duplicate marking in RNAseq data\n\n\n\nBy default, the nf-core/rnaseq pipeline uses picard to mark the duplicate reads identified amongst the alignments to allow you to guage the overall level of duplication in your samples. Unless you are using unique molecular identifiers (UMIs) it is not possible to establish whether your duplicate sequence reads are true biological duplication or PCR bias introduced during the library preparation.\nGiven our data hasn‚Äôt been tagged with UMIs, we won‚Äôt be able to accurately remove artifact duplicates from the alignments. Therefore we don‚Äôt need to worry about tagging them.\n\n\n‚û§ Execute the following:\nnextflow run nf-core-rnaseq_3.12.0/3_12_0/main.nf \\\n    --input ~/Data/samplesheet.csv \\\n    --outdir WBS-mouse-results \\\n    --fasta ~/Data/mm10_reference/mm10_chr18.fa \\\n    --gtf ~/Data/mm10_reference/mm10_chr18.gtf \\\n    --star_index ~/Data/mm10_reference/STAR \\\n    --salmon_index ~/Data/mm10_reference/salmon-index \\\n    -profile singularity \\\n    --skip_markduplicates \\\n    --max_memory '6.GB' \\\n    --max_cpus 2 -resume\n\n\n\n\n\n\nA note on reference genome indexing\n\n\n\nToday we are using a pre-indexed reference genome (--star_index and --salmon_index) we prepared for you. Index files are required for tools like our aligners to run faster. Think of it like the index at the back of a textbook, where you can easily find what pages certain words are on.\nNote that indexing requires a lot of memory and can be slow, so if you didn‚Äôt have a prepared index and you needed nf-core/rnaseq to perform indexing, you would need to increase memory (--max_memory).\n\n\nThe progress of the workflow will be displayed in the terminal and updated in real-time. Take a look at all the processes you have just run with a single command!"
  },
  {
    "objectID": "notebooks/1.3_nfcore-run.html#customise-the-run-command",
    "href": "notebooks/1.3_nfcore-run.html#customise-the-run-command",
    "title": "Data pre-processing with nf-core/rnaseq",
    "section": "1.3.2 Customise the run command",
    "text": "1.3.2 Customise the run command\nThe nf-core/rnaseq pipeline can be run using a single command with default parameters. The required parameters outlines a few basic requirements:\n\nAn input sample sheet (--input)\nA reference genome, either one available through Illumina‚Äôs iGenomes database (--genome), or a user-specified reference assembly (--fasta) and annotation file (--gtf)\nA configuration profile suitable for the computing environment you‚Äôre working on (-profile).\n\n\nCompared with the run command we executed, a user working with human samples, who has Singularity installed and all fastq files stored in a directory would be able to run this command to run the nf-core/rnaseq pipeline:\nnextflow run nf-core/rnaseq \\   \n    --input samplesheet.csv \\  # Samples and their fqs\n    --genome GRCh38 \\          # Illumina iGenomes database\n    -profile singularity       # For pre-installed software\nMost of us will need to customise the command a little more than this though, depending on the resources available on our system, our dataset, and our analytical needs. Take a look at the nf-core/rnaseq usage instructions to determine what is right for you.\n\n\n\n\n\n\nHints for running nf-core on your infrastructure\n\n\n\n\n\nNextflow allows us to set our configuration needs within a custom configuration file and apply it to our execution command using the -c flag.\nSomeone may have already built and shared a configuration file specifically for your infrastructure, like this config we built for Pawsey‚Äôs Nimbus instances. These pre-built, infrastructure specific configs are called Institutional configs and they can be downloaded with the pipeline code base and applied to all nf-core pipelines.\nInstitutional configs help us create efficient workflows that can be shared with others to reproducibly run the workflow in the same computational environment.\n\n\n\nOnce the pipeline has successfully completed, you will again see a message printed to your terminal that outlines how long it took your pipeline to run, how many tasks were executed, and how many of those tasks were cached:\n-[nf-core/rnaseq] Pipeline completed successfully -\nCompleted at: 29-Sep-2023 03:55:58\nDuration    : 15m 12s\nCPU hours   : 0.4 (14.5% cached)\nSucceeded   : 153\nCached      : 17\n‚û§ We will explore the outputs in depth in the next lessons. For now, list out the contents of your output directory:\nls -lh WBS-mouse-results\ntotal 28K\ndrwxrwxr-x  7 training training 4.0K Sep 29 03:55 .\ndrwxrwxr-x  9 training training 4.0K Sep 29 03:40 ..\ndrwxrwxr-x  2 training training 4.0K Sep 29 03:41 fastqc\ndrwxrwxr-x  3 training training 4.0K Sep 29 03:55 multiqc\ndrwxrwxr-x  2 training training 4.0K Sep 29 03:55 pipeline_info\ndrwxrwxr-x 17 training training 4.0K Sep 29 03:55 star_salmon\ndrwxrwxr-x  3 training training 4.0K Sep 29 03:41 trimgalore\n\n\n\n\n\n\nKey takeaways\n\n\n\n\nThe nf-core/rnaseq pipeline offers users a high degree of flexibility and customisation whilst enhancing reproducibility.\nUsers can modify parameters, customise configurations to tailor the pipeline‚Äôs execution to the needs of their compute environment, experimental design, and research objectives.\nNextflow and nf-core pipelines employ a cache system for workflow execution, allowing users to resume workflows from any point of failure by using the -resume flag."
  },
  {
    "objectID": "notebooks/2.1_rstudio-downstream.html",
    "href": "notebooks/2.1_rstudio-downstream.html",
    "title": "Set up R/RStudio for downstream analyses",
    "section": "",
    "text": "Learning objectives\n\n\n\n\nExecute an RStudio container using Singularity\nLearn how to import a multi-sample gene count matrix into RStudio using R code"
  },
  {
    "objectID": "notebooks/2.1_rstudio-downstream.html#what-is-r-and-rstudio",
    "href": "notebooks/2.1_rstudio-downstream.html#what-is-r-and-rstudio",
    "title": "Set up R/RStudio for downstream analyses",
    "section": "2.1.1 What is R and RStudio?",
    "text": "2.1.1 What is R and RStudio?\nR is a programming language for statistical computing and graphics. It is one of the most widely-used programming languages used in data science. R is a particularly great option when we‚Äôre working with a variety of statistical tools (e.g.¬†RNA-Seq, population genomics, etc.) and want to create publication-quality graphs and figures. Multiple independant packages/libraries have been developed in R-programming, which can be used for performing various kinds of ‚Äòomics‚Äô analysis. RStudio is a development environment which can be used to write and excecute R code.\n\nUsing R/RStudio for differential expression analysis\nWe‚Äôll be using the raw gene count matrix output by the nf-core/rnaseq pipeline to perform statistical analyses and determine differentially expressed (DE) genes and pathways. As we‚Äôre working with R/RStudio, we‚Äôll be using a number of different R packages to evaluate, analyse and visualise our data. A number of R packages have been developed specifically to identify differencially expressed (DE) genes, today we‚Äôre using the DESeq2 package.\nWe‚Äôve created a Singularity container that houses R/Rstudio with all the libraries we‚Äôll be using today already pre-installed. We‚Äôll be using that container to run RStudio on Nimbus."
  },
  {
    "objectID": "notebooks/2.1_rstudio-downstream.html#run-the-rstudio-container-on-nimbus",
    "href": "notebooks/2.1_rstudio-downstream.html#run-the-rstudio-container-on-nimbus",
    "title": "Set up R/RStudio for downstream analyses",
    "section": "2.1.2 Run the RStudio container on Nimbus",
    "text": "2.1.2 Run the RStudio container on Nimbus\nWhile you may be familiar with RStudio as a desktop application, it also available as a remote server on Nimbus. Launching an RStudio server from Nimbus allows us to run RStudio from a web browser and this has a few advantages.\n\nWe don‚Äôt have to download the RStudio desktop application to our own computer.\nWe don‚Äôt need to switch between machines and continue working on the Nimbus VM instance.\n\nTo run RStudio in a web browser, we first need to return to our Nimbus instances and run the RStudio server command. Log into Nimbus, same as you did for Day 1.\nPawsey have created a ‚Äòbio image‚Äô specifically for Nimbus that caters to bioinformatics users who prefer to have their instances set up with popular bioinformatic tools. You can learn more about it here. We‚Äôve been using this bio image for the workshop, as it has Nextflow and Singularity pre-installed for us üòä.\nChange into your working directory\ncd ~/Day-2/\n‚û§ On your Nimbus instance, create a rstudio-server folder on your instance:\nmkdir -p /tmp/rstudio-server\nSet a password for your RStudio server:\nRSERVER_PASSWORD=$(openssl rand -base64 15)\necho $RSERVER_PASSWORD\nPlease notedown this PASSWORD as you will need when you log on to RStudio from a browser.\nWe‚Äôre then going to run a Singularity command to execute the prepared RStudio container containing all the libraries. This command looks a bit intimidating, but all you really need to know to get it working is:\n\nWe‚Äôve set a unique password (PASSWORD) that we‚Äôll use to log into the RStudio server on our web browsers\nWe are using Singularity to execute (singularity exec) the a RStudio container (/cvmfs/data.biocommons.aarnet.edu.au/training_materials/SIH_training/IntroRNAseq_1023/rstudio_4.1.0.sif)\nContainers are closed environments, so we have to explicitly bind (-B) or expose directories that we want it to access on our machine.\n\n‚û§ To start the Rserver, run:\nPASSWORD=$RSERVER_PASSWORD singularity exec \\\n    -B /tmp/rstudio-server:/var/lib/rstudio-server \\\n    -B /tmp/rstudio-server:/var/run/rstudio-server \\\n     -B /home/training/Day-2:/home/training/ \\\n    ~/Data/rstudio_4.1.0.sif \\\n    rserver --auth-none=0 --auth-pam-helper-path=pam-helper --server-user training\n\n\n\n\n\n\nNo output is good output üòÜ\n\n\n\nOnce you paste the above command in the terminal and press Enter, the terminal SHOULD NOT return anything. If this happens, we are good to go!\n\n\n\n\n\n\n\n\nFor experienced CLI users\n\n\n\nYou can also run the above command using the screen utility, as we did yesterday.\n\n\n\nOpen RStudio in a web browser\n‚û§ Open up a web browser on your local machine. Remember, you will need to use Chrome or Safari for this. Participants using Firefox may experience technical issues.\nType 146.118.XX.XX:8787 in your browser where the XX.XX will be replaced by your IP specific digits:\n\nWhen the RServer interface launches, sign in to RStudio. Enter ‚Äòtraining‚Äô in the username text box and the [PASSWORD] which you had noted down previously in the password text box:\n\nIf you forgot to note down your password, you can retreive it by running the command: echo $RSERVER_PASSWORD\nIf you need to end your session, exit your browser and on Nimbus, run the following command:\nkill -9 $(lsof -t -c rsession)"
  },
  {
    "objectID": "notebooks/2.1_rstudio-downstream.html#working-in-rstudio",
    "href": "notebooks/2.1_rstudio-downstream.html#working-in-rstudio",
    "title": "Set up R/RStudio for downstream analyses",
    "section": "2.1.3 Working in RStudio",
    "text": "2.1.3 Working in RStudio\n\nOpen the analysis file\n‚û§ In RStudio, go to File > Open File ‚Ä¶ > Day-2 > rnaseq_DE_analysis_Day2.Rmd.\nYou should have something similar to what‚Äôs in the image below. We will be running our analysis from this file.\n\n\n\nUsing R/Rstudio and Rmarkdown\nFor this part of the workshop, we will be writing and running code in an R Markdown (.Rmd) file format. R Markdown combines R code (analysis) with Markdown code (yes - another language!). This will allow us to run code in chunks. At the end of our analysis, we will create a nice HTML report containing our results and commentary.\nR Markdown is very simple - all you need to know for now is:\n\nThe grey blocks contain R code, run the code by clicking the green arrow ‚ñ∂Ô∏è on the top right. * Within the code blocks:\n\nComment lines begin with ‚Äú#‚Äù, bullet points begin with ‚Äú-‚Äù\n\nRun code blocks in sequential order\nPretty much everything else outside these boxes is Markdown format. This is mainly instructions or an explanation of the R code blocks that follow:\n\nFeel free to add your own notes and save a copy of this file\n\n\nFor more details on using R Markdown see here.\n\n\n\n\n\n\nWe are only working from the .Rmd!!\n\n\n\nThis webpage will contain the same R code as is in rnaseq_DE_analysis_Day2.Rmd. You are welcome to use and modify this webpage‚Äôs code for your own future experiments, but during today, we will be working from RStudio and the .Rmd file only."
  },
  {
    "objectID": "notebooks/2.1_rstudio-downstream.html#prepare-your-r-environment",
    "href": "notebooks/2.1_rstudio-downstream.html#prepare-your-r-environment",
    "title": "Set up R/RStudio for downstream analyses",
    "section": "2.1.4 Prepare your R environment",
    "text": "2.1.4 Prepare your R environment\nBefore we begin, we need to load our libraries and set our working directory to make sure all the files we need are in the right place.\n\nLoad libraries\n‚û§ Click the green arrow in the top right of the R grey code box to run the code chunk. This will load libraries required for this analysis.\nsuppressMessages({\nlibrary(\"DESeq2\")\nlibrary(\"edgeR\")\nlibrary(\"limma\")\nlibrary(\"RColorBrewer\")\nlibrary(\"gplots\")\nlibrary(\"ggplot2\")\nlibrary(\"factoextra\")\nlibrary(\"devtools\")\nlibrary(\"rstudioapi\")\nlibrary(\"dplyr\")\nlibrary(\"tibble\")\nlibrary(\"tidyverse\")\nlibrary(\"pheatmap\")\nlibrary(\"biomaRt\")\nlibrary(\"annotables\")\nlibrary(\"org.Mm.eg.db\")\nlibrary(\"biobroom\")\nlibrary(\"clusterProfiler\")\nlibrary(\"ggnewscale\")\n})\n\n\nImport the count matrix file\nThe count matrix file contains raw (un-normalized) counts for every gene (rows) in mm10 and every sample (columns). The value in the i-th row and the j-th column of the matrix tells how many raw reads were assigned to gene i in sample j.\nWe created a count matrix file with subset data in Part 1 of this workshop (salmon.merged.gene_counts.tsv). In Part 2, we will use a count matrix file produced from the full dataset, in order to have enough data to generate differential pathways.\n# Read in the full count matrix file\ncounttable_original<-read.delim(\"FULL_count_matrix.txt\", \n                                header=T, \n                                row.names=1)\n\n# Data format is very important to ensure that functions read and analyse data correctly!\n# The loaded count matrix is not in the exact format as required by the functions used later in the analysis\n# So we perform the following steps\n\n# Put gene symbol in the first column\ncounttable <- counttable_original[,c(\"Symbol\",\"WT1\",\"WT2\",\"WT3\",\"KO1\",\"KO2\",\"KO3\")]\n\n# We don't need the ensembl IDs - get rid of the rownames\nrow.names(counttable) <- NULL\n\n# Make the gene symbol column rownames instead\nrownames(counttable) <- counttable$Symbol\ncounttable <- counttable[,c(\"WT1\",\"WT2\",\"WT3\",\"KO1\",\"KO2\",\"KO3\")]\n\n#  View the count table with the code below\nView(counttable)\nWe now have our count matrix file (genes - rows, samples - columns) ready for analysis."
  },
  {
    "objectID": "notebooks/2.1_rstudio-downstream.html#experimental-design-metadata",
    "href": "notebooks/2.1_rstudio-downstream.html#experimental-design-metadata",
    "title": "Set up R/RStudio for downstream analyses",
    "section": "2.1.5 Experimental design metadata",
    "text": "2.1.5 Experimental design metadata\nDE requires some metadata that tells our R libraries about the experimental design of the study, so that it knows how to handle the data. In this analysis, we have two experimental groups, the wildtype (‚ÄúWild‚Äù) and the knockout (‚ÄúKO‚Äù) groups.\nWe will create and store this metadata in a specific format required by the R libraries that we will use later. The samples are in rows (sample IDs as rownames), and columns are the experimental groupings. You can have more than one column, but need a minimum of one that describes your experimental groups.\n# Define a condition variable, ensuring they match the order of sample IDs in counttable\ncondition = c(\"Wild\",\"Wild\",\"Wild\",\"KO\",\"KO\",\"KO\")\n\n# Create a dataframe called meta with condition and sample IDs as rownames (taken from counttable)\nmeta <- data.frame(row.names = colnames(counttable), condition)\n\n# View the meta dataframe with the code below\nView(meta)\n\n\n\n\n\n\nKey points\n\n\n\n\nUsing the RStudio server allows us to stay on the command line where all our files are, rather than download them to our desktops."
  },
  {
    "objectID": "notebooks/1.1_rnaseq-experiment.html",
    "href": "notebooks/1.1_rnaseq-experiment.html",
    "title": "RNAseq data pre-processing on the command-line",
    "section": "",
    "text": "Learning objectives\n\n\n\n\nUnderstand the basic RNAseq for differential expression data processing and analysis workflow\nUnderstand why Nextflow and nf-core are good options for reproducible and portable bioinformatics workflows\nComprehend the nf-core/rnaseq pipeline structure and run command"
  },
  {
    "objectID": "notebooks/1.1_rnaseq-experiment.html#what-is-rnaseq",
    "href": "notebooks/1.1_rnaseq-experiment.html#what-is-rnaseq",
    "title": "RNAseq data pre-processing on the command-line",
    "section": "1.1.1 What is RNAseq?",
    "text": "1.1.1 What is RNAseq?\nRNAseq experiments involve the collection of RNA from a tissue of interest, converting this raw biological material to a digitised format using a high-throughput sequencing platform, pre-processing the resulting raw sequence reads so they can then be analysed to identify differentially expressed genes and functional pathways.\nUsing this technique, we can measure gene expression at the transcriptome level by:\n\nCapturing a representative sample of mRNA in our samples\nAligning sequence reads to a reference genome\n\nIdentifying the number of sequence reads that are aligned to each gene\n\nApplying statistical methods to capture the variance and significance in gene expression levels across different conditions or groups of samples.\n\nThis method allows for the identification of upregulated or downregulated genes, giving insights into the biological processes and molecular functions that are active in the tissue being studied. In this workshop, by comparing the expression levels of genes between knock-out and wild type mice, it is possible to identify the molecular mechanisms underlying WBS and potentially discover targets for therapeutic intervention."
  },
  {
    "objectID": "notebooks/1.1_rnaseq-experiment.html#rnaseq-experimental-workflow",
    "href": "notebooks/1.1_rnaseq-experiment.html#rnaseq-experimental-workflow",
    "title": "RNAseq data pre-processing on the command-line",
    "section": "1.1.2 RNAseq experimental workflow",
    "text": "1.1.2 RNAseq experimental workflow\nThe experimental workflow for RNAseq is presented in the image below. It consists of six steps:\n\nIsolate RNA from samples\nPrepare and sequence the RNA\nPerform quality control of raw sequence reads\nAlign sequence reads to a reference genome and count the number of reads mapped to each gene\nApply statistical methods to capture variance and significance between groups\nIdentify which biological processes, components, and functions are over-represented"
  },
  {
    "objectID": "notebooks/1.1_rnaseq-experiment.html#rnaseq-data-processing-pipelines",
    "href": "notebooks/1.1_rnaseq-experiment.html#rnaseq-data-processing-pipelines",
    "title": "RNAseq data pre-processing on the command-line",
    "section": "1.1.3 RNAseq data processing pipelines",
    "text": "1.1.3 RNAseq data processing pipelines\nIn the first session/day of this workshop, we will be performing steps 3 and 4 of the experimental workflow above. We will then analyse our processed data in the second session/day of the workshop, following steps 5 and 6. To process our data on day 1 we will be:\n\nWorking on the command-line interface\nWorking with the raw sequence data from the Corley et al.¬†(2016) case study\nRunning the nf-core/rnaseq pipeline\n\n\n\n\n\n\n\nReflection exercise\n\n\n\nHere are all the steps that make up a typical differential expression workflow when we‚Äôre working with bulk RNA-seq data. Can you place the steps in the right order?\n\n\n\n\n\n\n\n\n\nSolution"
  },
  {
    "objectID": "notebooks/1.1_rnaseq-experiment.html#introduction-to-nf-corernaseq",
    "href": "notebooks/1.1_rnaseq-experiment.html#introduction-to-nf-corernaseq",
    "title": "RNAseq data pre-processing on the command-line",
    "section": "1.1.4 Introduction to nf-core/rnaseq",
    "text": "1.1.4 Introduction to nf-core/rnaseq\nnf-core is a community-curated collection of bioinformatics pipelines written in Nextflow. The nf-core community is global, comprising bioinformaticians, computational biologists, software engineers, and biologists. The community works together to develop and maintain best practice bioinformatics pipelines and support others in running them. They‚Äôve also developed a toolkit to assist in pipeline usage and development. Everyone is welcome to join the community!\nThe nf-core/rnaseq pipeline can be used to analyse RNA sequencing data obtained from organisms with a reference genome and annotated gene dataset. It is flexible and modular, allowing users to choose which processes to run and giving users the choice of different tools for essential steps like read alignment.\n\n\n\n\n\n\nWhy are we running a pipeline?\n\n\n\n\n\nBioinformatics workflows are like wet-lab protocols, they consist of many steps that need to be performed consistently across experiments. And just like wet-lab protocols, things can get very complicated very quickly when we‚Äôre working with different numbers of samples, different organisms, and collaborating with other researchers. Just take a look at the image below to see how messy things can get!\n\n\n\n\n\nWhat can nf-core/rnaseq do?\nThe nf-core/rnaseq pipeline is designed to handle data processing steps that can easily be standardised and automated. It is used to process the raw sequence reads and generate read count data that we will be analysing tomorrow. These steps are often the most computationally challenging to perform. Users can run their data through the whole pipeline with one command, or specific stages.\nLooking at the nf-core/rnaseq pipeline structure below, we can see that the developers have:\n\nOrganised the workflow into 5 stages based on the type of work that is being done\nProvided a choice of multiple methods and specified defaults\nProvided a choice of tool for some steps\n\n\n\n\n\n\n\n\nWhere can I run the pipeline?\n\n\n\n\n\nGiven Nextflow‚Äôs focus on portability and reproducibility, the nf-core/rnaseq pipeline can be run on any compute environment where you can install Nextflow and one of the software management tools like Singularity, Docker, or Conda (among others). Take a look at their installation guide for more details.\nKeep in mind that each of the tools the nf-core/rnaseq pipeline uses has its own minimum compute resource requirements. This means you‚Äôll need to make sure the environment you‚Äôre working on has enough RAM and CPUs to process your data and disk space to store your raw data, intermediate files, and final results. Given how greedy these processes can be, we usually recommend you don‚Äôt work on your personal computer."
  },
  {
    "objectID": "notebooks/1.1_rnaseq-experiment.html#familiarise-yourself-with-your-environment",
    "href": "notebooks/1.1_rnaseq-experiment.html#familiarise-yourself-with-your-environment",
    "title": "RNAseq data pre-processing on the command-line",
    "section": "1.1.5 Familiarise yourself with your environment",
    "text": "1.1.5 Familiarise yourself with your environment\n‚û§ Let‚Äôs take a look at our home directory on the command-line on your Nimbus instance.\nFirst, ensure you are logged into your instance. Follow set up instructions to log back into your instance in either VS Code or your terminal.\nIn VS Code:\n\nType Ctrl + Shift + P to open command palette and select Remote-SSH: Connect to Host and nfcoreWorkshop\nType in your provided password (see the Slack channel for the password) and hit enter\n\nOR\nIn a terminal application:\n\nType the following into your terminal, using your allocated instance‚Äôs IP address:\n\nssh training@146.118.XXX.XX\n\nType in your provided password (see the Slack channel for the password) and hit enter\n\nConfirm you are in your home directory by running:\npwd\n/home/training\nIf you are not in /home/training, move there by running:\ncd ~\nFrom your home directory, run the following command:\nls -lh\ndrwxrwxr-x 2 training training 4.0K Sep 29 02:14 Data\ndrwxrwxr-x 2 training training 4.0K Sep 29 02:14 Day-1\ndrwxrwxr-x 3 training training 4.0K Sep 29 03:15 Day-2\nAll the files necessary to run today‚Äôs exercises have already been prepared for you and saved to the Data directory. Take a look at its contents by running:\nls -lh Data\nIt contains the following:\n\nsamplesheet.csv\nReference genome (fasta) and indexes\nFastq files\nBack-up qc run results\nBack-up full run results\nComplete count matrix to be used on Day 2\nThe dowstream analysis R notebook to be used on Day 2\nThe downstream analysis RStudio Singularity file to be used on Day 2\n\nNow, move into the Day-1 directory:\ncd /home/training/Day-1\nYou will do all your work from here today.\n‚û§ Next, take a look at the contents of the Day-1 directory:\nls -lh \ndrwxrwxr-x 3 training training 4.0K Sep 29 03:15 nf-core-rnaseq_3.12.0\nWe have predownloaded the code-base for the nf-core/rnaseq pipeline. For the sake of responsible research and reproducibility, we recommend always downloading a copy of the code base when working with nf-core pipelines.\n\nGet familiar with the nf-core/rnaseq run command\n‚û§ You can see the recommended (typical) run command and all the parameters available for the nf-core/rnaseq pipeline either at their parameters page or by running:\nnextflow run nf-core-rnaseq_3.12.0/3_12_0/main.nf --help \nYou will see a message displayed on your screen with the name of the nf-core/rnaseq pipeline, a recommended run command, and a big list of parameters.\n\nLook at all the different parameters! We won‚Äôt be using most of these today as we‚Äôll be running the default workflow. nf-core has extensive documentation that you should always use to determine whether it is suitable for your applications, construct a run command, and understand its outputs.\nAs you can see from the typical run command printed when you ran the help command, we need to provide the pipeline with some essential parameters and inputs. Our run command specifies some extra details that we will explore in depth later, but for now the minimum requirements for running the pipeline are:\n\nAn input sample sheet containing paths to our raw sequence reads (fastq files)\nA location to save the results directory to\nThe reference data we are working with\nA software management tool, we will be using Singularity.\n\n\n\n\n\n\n\nReflection exercise\n\n\n\nTake a look at the nf-core/rnaseq instructions on building a samplesheet. Compare our samplesheet to the nf-core example, what differences do you notice?\nView our prepared samplesheet.csv by running:\ncat ~/Data/samplesheet.csv\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nBoth sample sheets contain 6 samples. However, the example samplesheet consists of both single- and paired-end data for the control samples and TREATMENT_REP3 has been sequenced twice.\nOur sample sheet is more simple, with only single-end reads. Note that the column for the reverse-reads is empty in our sample sheet.\n\n\n\nNow that we are familiar with our working space and our pipeline, we are going to run the nf-core/rnaseq pipeline in two stages:\n\nRaw data quality control\nAlignment and quantification\n\nThe mouse genome is large and contains ~20,000 genes which would take hours to process. For the sake of expediency, we are running the pre-processing workflow on a subset of the whole mouse genome for both of these stages. Tomorrow we will provide you with a whole genome count matrix, to perform differential expression and enrichment analyses.\n\n\n\n\n\n\nKey takeaways\n\n\n\n\nRNAseq for differential expression experiments can be used to gain insights into active biological processes in the studied tissue.\nThe standard RNAseq experimental workflow comprises multiple steps performed in the wet lab and bioinformatically or computationally.\nThe nf-core/rnaseq pipeline is a powerful tool to process RNAseq data, as it offers a standardised and automated approach to handle computationally challenging tasks."
  },
  {
    "objectID": "setup.html",
    "href": "setup.html",
    "title": "Set up your computer",
    "section": "",
    "text": "In this workshop series, we will be using Pawsey‚Äôs Nimbus cloud. The Pawsey Supercomputing Research Centre is one of two, Tier-1, High Performance Computing facilities in Australia.\nThe main requirements for this workshop are a personal computer with:\n\nA web browser (Google Chrome is recommended)\nVisual Studio Code (recommended) or a terminal application\n\nBelow, you will find instructions on how to set up a terminal application and web browser on your computer and how to connect to Nimbus. Each participant will be provided with their instance‚Äôs IP address at the beginning of the workshop.\n\nOption 1: Install and set up Visual Studio Code (recommended)\nVisual Studio Code (VS Code) is a lightweight and powerful source code editor available for Windows, macOS and Linux computers.\n\nDownload Visual Studio Code for your system from here and follow the instructions for:\n\nmacOS\nLinux\nWindows\n\nOpen the VS Code application on your computer\n\n\n\nClick on the extensions button (four blocks) on the left side bar and install the remote SSH extension. Click on the blue install button.\n\n\n\nInstall the Live Server extension. Click on the blue install button.\n\n\n\nLogin via Visual Studio Code\nConnect to your instance with VS code by adding the host details to your .ssh config file:\n\nGo to the list of instance IP addresses provided in the Slack channel\nFind and copy your IP address, it will start with 146.118.XXX.XX\nIn a new VS code terminal, type Ctrl+Shift+P if you‚Äôre on a Windows machine or Cmd+Shift+P for MacOS to open the command palette\nSelect Select Remote-SSH: Open SSH configuration file and select your .ssh config file\nAdd a new entry using your allocated IP address and save your .ssh config file:\n\nHost rnaseqWorkshop\n  HostName 146.118.XXX.XX\n  User training\n\nType Ctrl+Shift+P and select Remote-SSH: Connect to Host and rnaseqWorkshop\nWhen prompted, select Linux as the platform of the remote host from the dropdown menu\nType in your provided password (see the Slack channel for the password) and hit enter\n\nHaving successfully logged in, you should see a small blue or green box in the bottom left corner of your screen:\n\nTo set up your VS Code window for the workshop:\n\nOpen a new folder in the file explorer panel on the left side of the screen by typing Ctrl + K, Ctrl + O if you‚Äôre running Windows or Cmd+K+ Cmd + O for MacOS\nSelect /home/training to open your instance‚Äôs $HOME folder, this is where we will be working\nWhen prompted, select the box for Trust the authors of all files in the parent folder ‚Äòhome‚Äô then click Yes, I trust the authors\nTo open a terminal, type Ctrl+J if you‚Äôre on a Windows machine or Cmd+J on MacOS\n\n\n\nTips for using VS Code\n\nVS code cheatsheet for Windows\nVS code cheatsheet for MacOS\n\n\n\n\n\n\n\n\n\nShortcut\nWindows\nMacOS\n\n\n\n\nShow command palette\nctrl+shift+P\ncmd+shift+P\n\n\nToggle sidebar\nctrl+B\ncmd+B\n\n\nOpen new window\nctrl+shift+N\ncmd+shift+N\n\n\nOpen/close terminal\nctrl+J\ncmd+J\n\n\nQuick file open\nctrl+P\ncmd+P\n\n\nZoom in\nctrl +\ncmd +\n\n\nZoom out\nctrl -\ncmd -\n\n\nFind\nctrl+F\ncmd+F\n\n\nSave\nctrl+S\ncmd+S\n\n\nSelect current line\nctrl+L\ncmd+L\n\n\nEdit every instance of highlighted string\nctrl+shift+L\ncmd+shift+L\n\n\n\n\n\n\n\nOption 2: Install and set up a terminal application\nThe terminal applications available to you will depend on your operating system.\n\nLinux terminals\nIf you use Linux, chances are you already know your shell and how to use it. Basically, just open your preferred terminal program and off you go!\n\n\nOS X (Mac)\nMac operating systems come with a terminal program, called Terminal. Just look for it in your Applications folder, or hit Command + Space and type ‚Äòterminal‚Äô. You may find that other, 3rd party terminal programs are more user-friendly and powerful, like Iterm2.\n\n\nWindows\nWe recommend MobaXterm, which offers a rich experience as a full-featured X-server and terminal emulator for ssh connections, the free version is more than adequate.\nTo install and start using MobaXterm:\n\nGo to https://mobaxterm.mobatek.net/download.html\nUnder ‚ÄòHome Edition‚Äô select the Download now button\nSelect the MobaXterm Home Edition (Installer edition)\nOnce the program is downloaded, install it as you would any other windows program\nOnce the program is installed, start the MobaXterm program\nFrom this screen, click on ‚Äòstart local terminal‚Äô (and install Cygwin if prompted)\n\n\n\n\nLogin via Terminal\nTo log in to Nimbus, we will use a Secure Shell (SSH) connection. To connect, you need 3 things:\n\nThe assigned IP address of your instance (i.e.¬†146.118.XXX.XX). Each participant will be provided with their instance‚Äôs IP address at the beginning of the workshop.\nYour login name. In our case, this will be training for all participants.\nYour password. All participants will be provided with a password at the beginning of the workshop.\n\nTo log in: type the following into your terminal, using your allocated instance‚Äôs IP address:\nssh training@146.118.XXX.XX\nYou will receive a message saying:\nThe authenticity of host '146.118.XX.XXX (146.118.XX.XXX)' can't be established.\nRemember your host address will be different than the one above. There will then be a message saying:\nAre you sure you want to continue connecting (yes/no)?\nIf you would like to skip this message next time you log in, answer ‚Äòyes‚Äô. It will then give a warning:\nWarning: Permanently added '146.118.XX.XXX' (ECDSA) to the list of known hosts.\nEnter the password provided at the beginning of the workshop. Ask one of the demonstrators if you‚Äôve forgotten it.\n\n\n\n\n\n\nPay Attention\n\n\n\nWhen you type a password on the terminal, there will not be any indication the password is being entered. You‚Äôll not see a moving cursor, or even any asterisks, or bullets. That is an intentional security mechanism used by all terminal applications and can trip us up sometimes, so be careful when typing or copying your password in.\n\n\nHaving successfully logged in, your terminal should then display something like that shown in the figure below:"
  },
  {
    "objectID": "notebooks/2.0_recap_day1.html",
    "href": "notebooks/2.0_recap_day1.html",
    "title": "Recap of Day 1",
    "section": "",
    "text": "Learning objectives\n\n\n\n\nLearn how to interpret an aggregated MultiQC report\nLearn how to interpret Nextflow workflow execution reports"
  },
  {
    "objectID": "notebooks/2.6_workshop_summary.html",
    "href": "notebooks/2.6_workshop_summary.html",
    "title": "Workshop summary",
    "section": "",
    "text": "On Day 1 we used the nf-core/rnaseq pipeline to convert raw RNAseq data into raw counts.\nWe discussed the following essential steps in a RNAseq analysis workflow:\n\nCheck the quality of raw-reads.\nTrim the raw-reads to get rid of bad-quality read-regions and/or bad-quality reads.\n\nAlign the trimmed-reads to reference sequence to identify where they belong\n\nQuantify the aligned reads to get gene-level read-counts.\n\nNext:\n\nWe discussed the workflow management tool nextflow which is used for automated, reproducible, flexible and portable analysis.\n\nWe excecuted the nf-core/rnaseq pipeline and interpreted its outputs."
  },
  {
    "objectID": "notebooks/1.4_alignquant.html",
    "href": "notebooks/1.4_alignquant.html",
    "title": "Explore read alignment and quantification outputs",
    "section": "",
    "text": "Learning objectives\n\n\n\n\nUnderstand the process of mapping sequencing reads to a reference genome in a splice-aware manner\nUnderstand how mapped reads are used to quantify gene counts\nIn this lesson we will explore the outputs of the nf-core/rnaseq pipeline in depth, looking specifically at the results of read alignment and quantification (red box below)."
  },
  {
    "objectID": "notebooks/1.4_alignquant.html#alignment-of-reads-to-a-reference-genome",
    "href": "notebooks/1.4_alignquant.html#alignment-of-reads-to-a-reference-genome",
    "title": "Explore read alignment and quantification outputs",
    "section": "1.4.1 Alignment of reads to a reference genome",
    "text": "1.4.1 Alignment of reads to a reference genome\nThe output we will be examining in this lesson is output by the second stage of the nf-core/rnaseq workflow: read alignment and quantification (red box below).\n\nAfter read trimming, the nf-core/rnaseq pipeline maps reads to a reference genome specified by the user using STAR. Keep in mind that RNAseq specific alignment tools are different from whole genome alignment tools. They are designed to be ‚Äòsplice aware‚Äô, meaning they are capable of differentiating intronic from exonic regions in the alignment process.\nBulk RNA-seq reads are derived from mature mRNA and contain only exons (and no introns). This means a sequence read can span 2 exons. Splice-aware aligners use a reference genome, not a transcriptome to perform read alignment, which contain intergenic, intronic and exonic sequences. This means that when they‚Äôre aligned to the reference genome, RNA-seq reads might span large introns. Splice aware aligners, like STAR know not to align the RNA-seq reads to introns and can align a read across exons.\n\n\n\n\n\n\nGenetics 101 refresher: mRNA splicing\n\n\n\nmRNA splicing is the process by which an mRNA transcript prepares to be translated into an amino acid sequence. It works by removing introns and splicing all exons back together to create a mature mRNA that is transported from the nucleus to the cytoplasm, ready to undergo translation.\n\n\nThe nf-core/rnaseq pipeline offers us various alignment and quantification routes:\n\nSTAR ‚Äì Salmon\nSTAR ‚Äì RSEM\nHISAT2 ‚Äì no quantification\n\nAligned sequences for each sample are output in the bam file format. STAR (Spliced Transcript Alignment to a Reference) is a very popular RNAseq alignment tool. It has previously been shown to be more accurate and efficient than other popular RNAseq alignment tools.\nThe nf-core/rnaseq pipeline has been written in a way where all the files generated from the alignment step onwards (except the summary report) are saved to the same directory as specified by --aligner. In our case, you will find the output alignment files in star_salmon:\nls -lh WBS-mouse-results/star_salmon/*.bam*\n-rw-rw-r-- 1 ubuntu ubuntu 3.7M Sep 29 03:45 SRR3473984.sorted.bam\n-rw-rw-r-- 1 ubuntu ubuntu  51K Sep 29 03:46 SRR3473984.sorted.bam.bai\n-rw-rw-r-- 1 ubuntu ubuntu 3.4M Sep 29 03:46 SRR3473985.sorted.bam\n-rw-rw-r-- 1 ubuntu ubuntu  50K Sep 29 03:47 SRR3473985.sorted.bam.bai\n-rw-rw-r-- 1 ubuntu ubuntu 3.3M Sep 29 03:46 SRR3473986.sorted.bam\n-rw-rw-r-- 1 ubuntu ubuntu  52K Sep 29 03:47 SRR3473986.sorted.bam.bai\n-rw-rw-r-- 1 ubuntu ubuntu 3.4M Sep 29 03:46 SRR3473987.sorted.bam\n-rw-rw-r-- 1 ubuntu ubuntu  53K Sep 29 03:48 SRR3473987.sorted.bam.bai\n-rw-rw-r-- 1 ubuntu ubuntu 4.3M Sep 29 03:46 SRR3473988.sorted.bam\n-rw-rw-r-- 1 ubuntu ubuntu  53K Sep 29 03:47 SRR3473988.sorted.bam.bai\n-rw-rw-r-- 1 ubuntu ubuntu 3.1M Sep 29 03:46 SRR3473989.sorted.bam\n-rw-rw-r-- 1 ubuntu ubuntu  49K Sep 29 03:48 SRR3473989.sorted.bam.bai\nNotice above we have both alignment files (.bam) and their index files (.bam.bai). Index files are commonly created for large bioinformatics files to allow for rapid querying. These index files are just like the index at the back of a book. They are used by software to quickly find the location of a specific region without having to scan the whole file.\n\n\n\n\n\n\nMarking duplicate reads in RNAseq data\n\n\n\nBy default, the nf-core/rnaseq pipeline uses picard‚Äôs MarkDuplicates to mark the duplicate reads identified amongst the alignments to allow you to guage the overall level of duplication in your samples.\nRecall that we applied the --skip_markduplicates parameter. This is because it is not recommended to remove duplicates from RNAseq data because you expect a significant level of true biological duplication that arises from the same fragments being sequenced repeatedly.\n\n\n‚û§ Let‚Äôs take a look at how our aligner and quantification tools performed. Take a look at the alignment scores in the MultiQC summary report generated by the pipeline, you‚Äôll find it at:\n/home/training/Day-1/WBS-mouse-results/multiqc/star_salmon/multiqc_report.html\n\n\nAs expected, our alignments performed well. Most of the reads in all of our samples were uniquely mapped to the mouse reference genome and only small proportion were unmapped."
  },
  {
    "objectID": "notebooks/1.4_alignquant.html#visualising-alignments",
    "href": "notebooks/1.4_alignquant.html#visualising-alignments",
    "title": "Explore read alignment and quantification outputs",
    "section": "1.4.2 Visualising alignments",
    "text": "1.4.2 Visualising alignments\nYou can visualise alignments with tools like the Integrative Genomics Viewer (IGV). Visualising alignments around genomic regions of interest can be a reliable way to identify potential problems like contamination, biases, or technical artefacts. Let‚Äôs take a quick look at how alignments work, using the IGV output below. IGV has helpful documentation regarding the visualisation of RNAseq alignments.\n\n‚û§ Take a look at the IGV image below. We have zoomed in to the first three exon in our gene of interest, Gtf2ird1. Looking at the read depth track, we can see the vast majority of our reads have aligned to exons (purple), but there are some tiny bumps between exons. This may be an indication of some DNA contamination or misalignment of reads in our samples. Nothing to be concerned about though.\n\n‚û§ Take a look at the IGV image below. It covers one exon of our gene of interest, Gtf2ird1. If you compare it to the annotated screenshots above, you can see we have zoomed into the region spanning just the first exon (chr5:134417034-134417200) in the mm10 assembly. We can see both the read depth and alignment tracks much clearer. Note that reads only span the exons, they do not continue into the introns. Ths is because the STAR aligner is splice-aware. This means it recognises and aligns reads across splicing junctions, allowing us to capture exon-exon junctions.\n\n\n\n\n\n\n\nChallenge exercise\n\n\n\nWe forgot to label the conditional groupings of our samples and we don‚Äôt know which samples belong to the wildtype and knockout groups! Can you use the alignment files above to assign samples to their correct treatment group?\nHint: From the case study, we know a loss of function mutation of Gtf2ird1 was generated by an insertion of a Myc transgene, resulting in a 40 kb deletion surrounding an exon.\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nNavigate to the gene in the mm10 assembly (chr5:134332897-134481480)\nSamples SRR3473984, SRR3473985, SRR3473984 contain reads covering exon 1. These are wildtype samples.\nSamples SRR3473987, SRR3473988, SRR3473989 DO NOT contain reads covering exon 1. These are knockout samples."
  },
  {
    "objectID": "notebooks/1.4_alignquant.html#read-quantification",
    "href": "notebooks/1.4_alignquant.html#read-quantification",
    "title": "Explore read alignment and quantification outputs",
    "section": "1.4.3 Read quantification",
    "text": "1.4.3 Read quantification\nFollowing read alignment, the nf-core/rnaseq pipeline uses the bam files to calculate raw gene-count data for each sample. We can then use these count files (called a gene-count matrix), to identify differentially expressed genes. By default, the nf-core/rnaseq pipeline runs Salmon for transcript quantification, but users have other quantification options.\nSalmon is another popular RNAseq tool, it can take a set of target transcripts to perform quantification. The primary output of Salmon is the quantification results, and these results are provided in different formats and levels of aggregation.\n‚û§ Take a look at the Salmon outputs for a single sample, SRR3473984:\nls -lh /home/training/WBS-mouse-results/star_salmon/SRR3473984\ntotal 248K\ndrwxrwxr-x  5 training training 4.0K Sep 29 03:46 .\ndrwxrwxr-x 17 training training 4.0K Sep 29 03:55 ..\ndrwxrwxr-x  2 training training 4.0K Sep 29 03:46 aux_info\n-rw-rw-r--  1 training training  265 Sep 29 03:46 cmd_info.json\ndrwxrwxr-x  2 training training 4.0K Sep 29 03:46 libParams\ndrwxrwxr-x  2 training training 4.0K Sep 29 03:46 logs\n-rw-rw-r--  1 training training  49K Sep 29 03:46 quant.genes.sf\n-rw-rw-r--  1 training training 172K Sep 29 03:46 quant.sf\nNote the quant.sf and quant.genes.sf files. quant.sf provides quantification at the transcript level. That is, each row in this file corresponds to a specific transcript, and the reported quantities are the estimated abundances of these transcripts.\nquant.genes.sf provides quantification at the gene level. That is, the abundances of individual transcripts are aggregated to their respective genes, and each row corresponds to a gene. This aggregation is useful for downstream analyses that require gene-level, rather than transcript-level, quantifications.\nMerged, multi-sample count matrix files (tab separated values or TSV format) for both gene and transcript level counts are also produced by the workflow.\nThese raw counts are not a simple count of depth of reads, there is a lot more to consider and each tool does this slightly differently (e.g.¬†how do you count reads spanning overlapping exons across two different genes?). See the Salmon documentation for an explanation of how Salmon does it - this count is an estimation.\n\nWhen to use quant.sf vs quant.genes.sf\nIf you are interested in differential transcript usage, alternative splicing, or transcript-level analyses, you would use the quant.sf file.\nIf you are performing gene-level differential expression analysis, pathway analysis, or other analyses that operate on a gene-centric level, the quant.genes.sf would be more appropriate.\nWe‚Äôll be working with the quant.genes.sf file. It is a tab-separated file containing the read counts for genes. Columns in this file are:\n\nGene name\nGene length\nEffective length after adjusting for biases\nTranscripts per million\nEstimated number of reads\n\n\n\n\n\n\n\nChallenge exercise\n\n\n\nCan you identify the final gene count matrix files created by STAR and Salmon in the nf-core/rnaseq pipeline output?\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThe count matrix files can be found here: WBS-mouse-results > star_salmon > salmon.merged.gene_counts.tsv. Most genes will have 0 counts because we subset the data.\n\n\n\n\n\n\n\n\n\nKey takeaways\n\n\n\n\nA variety of methods for read alignment and transcript quantification are available in the nf-core/rnaseq pipeline.\nSplice aware alignment tools like HISAT2 and STAR must be used when aligning RNA-seq reads to a reference genome."
  },
  {
    "objectID": "notebooks/1.2_nfcore-qc.html",
    "href": "notebooks/1.2_nfcore-qc.html",
    "title": "Raw read quality control with nf-core/rnaseq",
    "section": "",
    "text": "Learning objectives\n\n\n\n\nUnderstand how to apply parameters to an nf-core pipeline to cusomise its execution\nUnderstand the fastq file format\nLearn how to interpret a FastQC report for RNAseq data\nLearn about the benefits and drawbacks of read trimming for RNAseq-DE\nWe‚Äôre starting at the first bioinformatics stage of the RNAseq workflow, specifically with raw data QC step (red box below). When your data is sequenced, it will be output in the fastq format by the sequencing machine.\nThe nf-core/rnaseq pipeline uses FastQC to evaluate fastq files. FastQC provides a set of metrics which we can use to get a sense of whether our raw data has any problems that might impact our ability to analyse it downstream. This step corresponds to the first part (red box) of the nf-core/rnaseq pipeline."
  },
  {
    "objectID": "notebooks/1.2_nfcore-qc.html#run-the-qc-command",
    "href": "notebooks/1.2_nfcore-qc.html#run-the-qc-command",
    "title": "Raw read quality control with nf-core/rnaseq",
    "section": "1.2.1 Run the qc command",
    "text": "1.2.1 Run the qc command\nIn any bioinformatics experiment, it is crucial that you perform quality control (QC) on your data before you process it. Why?\n\nIdentify issues that may interfere with analysis and interpretation\nDetect biases which may have been introduced during library preparation or sequencing\n\nnf-core/rnaseq provides users the option to run FastQC and read trimming only. We will do this use the --skip_alignment flag/parameter. Using this flag will run 3 steps in the workflow:\n\nCheck quality of raw sequence reads for each sample with FastQC\nPerform raw read trimming with trim galore!\nCheck quality of trimmed reads for each sample with FastQC\nOutput a MultiQC report\n\n\n\n\n\n\n\nThe joys of open source software communities\n\n\n\nnf-core/rnaseq developers have only recently added this QC-only functionality into their pipeline code. Up until June 2023, you had to run the pipeline in full before you could examine the raw sequence QC outputs. After a number of users petitioned the developers (see this Github issue for our pleas), the developers added the --skip_alignment feature.\n\n\n‚û§ Run the following command to perform the QC steps above:\nnextflow run nf-core-rnaseq_3.12.0/3_12_0/main.nf \\\n    --input ~/Data/samplesheet.csv \\\n    --outdir WBS-mouse-QC \\\n    --fasta ~/Data/mm10_reference/mm10_chr18.fa \\\n    --gtf ~/Data/mm10_reference/mm10_chr18.gtf \\\n    --star_index ~/Data/mm10_reference/STAR \\\n    --salmon_index ~/Data/mm10_reference/salmon-index \\\n    -profile singularity \\\n    --skip_alignment \\\n    --max_memory '6.GB' \\\n    --max_cpus 2\nOnce the pipeline has been executed with the run command, you will see a message printed to the screen. This message contains a summary of the parameters you have used to execute the workflow.\n\nIn addition to this you will see a number of processes spawn out from the workflow. Your screen will progressively update as these tasks are completed. Once they have all finished, a completion message will be printed to your screen:\n-[nf-core/rnaseq] Pipeline completed successfully -\nCompleted at: 29-Sep-2023 09:17:28\nDuration    : 2m 31s\nCPU hours   : 0.1\nSucceeded   : 19"
  },
  {
    "objectID": "notebooks/1.2_nfcore-qc.html#evaluate-the-data-quality",
    "href": "notebooks/1.2_nfcore-qc.html#evaluate-the-data-quality",
    "title": "Raw read quality control with nf-core/rnaseq",
    "section": "1.2.2 Evaluate the data quality",
    "text": "1.2.2 Evaluate the data quality\n‚û§ Take a look at the output that is produced by your qc run:\nls -lh /data/Day-1/WBS-mouse-QC\ntotal 24K\ndrwxrwxr-x  6 training training 4.0K Sep 29 09:38 .\ndrwxrwxr-x 10 training training 4.0K Sep 29 05:51 ..\ndrwxrwxr-x  2 training training 4.0K Sep 29 09:37 fastqc\ndrwxrwxr-x  4 training training 4.0K Sep 29 09:38 multiqc\ndrwxrwxr-x  2 training training 4.0K Sep 29 09:38 pipeline_info\ndrwxrwxr-x  3 training training 4.0K Sep 29 09:37 trimgalore\nWe have 4 directories within our output directory. Using the File Explorer on VS code, open the WBS-mouse-QC directory and download the multiqc report multiqc/multiqc_report.html to your computer. This MultiQC report aggregates raw and trimmed data FastQC reports for each sample. Open this file in your web browser using Live Server and answer the questions below.\n\n\n\n\n\n\nOpening HTML files with Live Server in VS Code\n\n\n\n\nOpen your file explorer panel in VS Code\nOpen the WBS-mouse-QC directory\nOpen the multiqc directory\nRight click on multiqc_report.html\nSelect open with live server\n\nIf Live Server doesn‚Äôt work, right click on multiqc_report.html and select download.\n\n\n\n\n\n\n\n\nReflection exercise\n\n\n\nLooking at the Fastqc (raw) section of the MultiQC report:\n\nHow many unique and duplicate reads are in SRR3473989.fastq?\nWhat might be a reason for the high proportion of duplicate reads in this dataset?\nTake a look at the Per Base Sequence Content plot for all samples. Which part of the reads tend to have worse per base sequence quality?\nDo you think this dataset is of ‚Äògood‚Äô quality? Why or why not?\nDo you have any suggestions to improve the quality of our raw reads?\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n35,519 and 24,368\nRNAseq only captures transcripts, therefore the chances of observing duplicate reads is elevated.\nReads which tend to have worse per base sequence quality are towards the right hand side (3‚Äô end).\nThe color coding separates out regions of good quality (Red PhredQ > 28) from the rest. Overall yes, as most of the regions of the reads show quality values in red.\nWe can trim the bases towards the 3‚Äô-end and hope to improve the overall read-quality. But trimming by quality for RNA-seq data has its pros and cons.\n\n\n\n\n\n\n\n\n\n\nChallenge exercise\n\n\n\nFastQC was designed for whole genome sequencing (WGS) and not RNAseq experiments. With this knowledge, can you identify why some categories might have been marked as ‚Äúfail‚Äù or ‚Äúwarn‚Äù by looking at the HTML reports?\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nPer-base sequence content fails. Per sequence GC content, sequence duplication levels, and overrepresented sequences return warnings.\n\nPer-base sequence content fails because we always see bias at the start of RNA-seq reads, which tells us the random priming is not ‚Äòtruly random‚Äô. See here for a nice explanation of this.\n\nPer sequence GC content, sequence duplication levels, and overrepresented sequences return warnings are received for this same reason.\n\nGiven there is much less RNA sequence then DNA in our bodies, we don‚Äôt observe these biases in WGS.\n\nBy chance, RNA will be fragmented at the same spot and sequenced multiple times. For DNA, the purpose of these plots is to check for technical bias (optical duplicates - when the sequencer reads the same strand multiple times)."
  },
  {
    "objectID": "notebooks/1.2_nfcore-qc.html#read-trimming",
    "href": "notebooks/1.2_nfcore-qc.html#read-trimming",
    "title": "Raw read quality control with nf-core/rnaseq",
    "section": "1.2.3 Read trimming",
    "text": "1.2.3 Read trimming\nTrimming is sometimes performed to improve the quality of the raw data and potentially improve its mapability when it is being aligned to a reference genome. There are several ways to perform trimming:\n\nRemoval of poor quality reads or bases (e.g.¬†ends of reads)\nRemoval of adapter sequences\nRemoval of polyA tails\n\nThe nf-core/rnaseq pipeline uses Trim Galore for read quality trimming. It is able to perform quality-based removal of low-quality bases and adapter trimming. Given trimming can result in some reads being significantly shortened (sometimes to 0bp!), Trim Galore will filter reads that are too short to be used in downstream processes like read alignment. We are still looking at the outputs from the first part of the nf-core/rnaseq workflow below (red box).\n\n\nDoes trimming help?\nRead trimming is not always a necessary step when processing next generation sequencing (NGS) data. These days, NGS data is of a very high quality and the tools we use to perform processes like read mapping are capable of handling poor quality reads and adapter sequences.\nWhile the trimming adapter sequences has been shown to increase the quality of RNA-seq data (Dozmorov et al., 2015), other studies have shown that trimming of poor quality reads can effect gene expression estimates (Williams at el., 2016).\nWhen making the decision to trim your reads for differential expression RNAseq studies, we suggest following the recommendations of the read alignment tool you‚Äôll be using. We will explore trimming outcomes.\n‚û§ Open your Nimbus terminal again to do the challenge exercise below. Navigate to the results directory either on the command line or using your VS Code File Explorer and answer the following questions:\ncd ~/Day-1/WBS-mouse-QC\n\n\n\n\n\n\nReflection exercise\n\n\n\n\nWhich tool does nfcore-rnaseq use for read-trimming?\nWhich tool did you use to generate quality reports before and after trimming?\nWhat effect did trimming have on SRR3473989.fastq?\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nTrim galore.\nFastQC generates .html reports.\nTotal number of sequences have reduced after trimming. Read length is now 21 - 101. Per base sequence quality now mostly in the green.\n\nOpen trimgalore report by running:\ncat WBS-mouse-QC/trimgalore/SRR3473984.fastq.gz_trimming_report.txt\n\n\n\n\n\n\n\n\n\nKey takeaways\n\n\n\n\nQuality control is fundamental in RNAseq experiments to identify any issues that might skew analysis and interpretation.\nThe Fastq file format is the primary format used for raw sequencing data. Each entry in a Fastq file has a sequence identifier, the nucleotide sequence itself, a separator, and a base call quality score (Phred score) for each nucleotide.\nRead trimming is a process where certain portions of the raw sequence data are removed to enhance its quality and improve its mappability during alignment. The decision to trim reads in an RNAseq experiment should be based on the recommendations of the alignment tool used and the quality of your data."
  },
  {
    "objectID": "notebooks/2.5_wrapup.html",
    "href": "notebooks/2.5_wrapup.html",
    "title": "Day 2 wrap up",
    "section": "",
    "text": "In this session, we used a multi-sample gene count matrix generated by the nf-core/rnaseq pipeline to identify differentially expressed genes and functional enrichments. We worked in RStudio and performed exploratory data analysis, differential expression analysis, and functional enrichment analysis. In this lesson we will reflect on our results."
  },
  {
    "objectID": "notebooks/2.5_wrapup.html#takeaways",
    "href": "notebooks/2.5_wrapup.html#takeaways",
    "title": "Day 2 wrap up",
    "section": "Takeaways",
    "text": "Takeaways\n\n2.5.1 Exploratory analysis\nThe principle component analysis shows a good separation of the sample across conditions:\n\n\n\n2.5.2 Differential expression analysis\nGtf2ird1 KO showed dysregulation of many genes and many functionally enriched gene ontologies (GO).\nTotal 19,859 genes in mouse genome.\n- LFC &gt; 0 (up): 1,353 DE genes (6.8%)\n- LFC &lt; 0 (down): 984 DE genes (5%)\n\n\n\n2.5.3 Functional enrichment analysis\nTotal GO enrichments identified from DE genes.\n- Categories form Up-regulated genes : 823\n- Categories form Down-regulated genes : 255\n\nThe most significantly enriched GO terms captured high-level biological functions that weren‚Äôt all related to our phenotypes of interest. We will go back to the biology to make sense of all these results.\nWhich ontologies in the list are relevant to your experiment\nCraniofacial development\nDisorders relate to the bones of the skull (cranio) and face. Lead to distinctive facial features‚Äã\nGO: Wnt pathway\nInvolved in\n- cytoskeletal dynamics and cell adhesion\n- promote the differentiation of skin epithelial cells and the development of hair follicles\nGO: skin development, epidermis development\nCardiovascular abnormalities\nGO: striated muscle tissue development\nGO: muscle system process, muscle cell development, muscle cell differentiation\nGO: sarcomere organisation\n\n\n2.5.4 Experimental considerations / limitations.\n\nExperimental design: Choose the right tissue for the research question.\n\nNumber of replicates: To improve statistical power and overcome any problems due to outliers.\n\nRealistic interpretation of the results.\n\n\n\n2.5.5 Reproducible analysis using Rstudio with Rmarkdown\nWe used Singularity containers for portable and reproducible analysis"
  },
  {
    "objectID": "notebooks/1.6_wrapup.html",
    "href": "notebooks/1.6_wrapup.html",
    "title": "Day 1 wrap up",
    "section": "",
    "text": "In this session, we performed the necessary data pre-processing steps required for differential expression with RNAseq data. We used the nf-core/rnaseq pipeline to process raw data from fastq inputs, align sequence reads, generate gene counts and perform extensive quality control. This pipeline is built using Nextflow, which is a bioinformatics workflow management tool that supports reproducible, portable, and scalable analyses. Tomorrow, we will use the count matrix generated by this pipeline to identify differentially expressed genes and perform functional enrichment analysis. We will be working interactively with our data in RStudio."
  },
  {
    "objectID": "notebooks/1.6_wrapup.html#key-takeaways",
    "href": "notebooks/1.6_wrapup.html#key-takeaways",
    "title": "Day 1 wrap up",
    "section": "Key takeaways",
    "text": "Key takeaways\n\nRaw data quality control is an essential step, it allows you to identify any potential issues that may interfere with analyses.\nThe nf-core/rnaseq pipeline follows community best practices and offers a reproducible, portable, and user friendly method for pre-processing your RNAseq data.\nFastQC is a useful tool for fastq quality inspection, however it was not built for RNAseq data so your RNAseq data will always fail some of their tests.\nRead trimming is not always necessary. Your choice to trim reads will depend on their quality, presence of adapter sequences, and read alignment tool of choice. Over-trimming or unnecessary trimming can sometimes remove valuable sequence information, so it‚Äôs essential to strike a balance.\nRNAseq read alignment is splice-aware, meaning that it‚Äôs designed to identify and handle reads that span exon-exon junctions. This capability ensures that such reads are accurately mapped, providing a true representation of the transcriptome.\nRead quantification to create a count matrix for your samples is required for differential expression analysis. Proper quantification will ensure that RNA abundance is accurately represented.\nDifferent alignment and quantification tools and methods suit different applications. These tools will have different underlying models and assumptions that will be reflected in their outputs."
  },
  {
    "objectID": "notebooks/1.6_wrapup.html#useful-resources",
    "href": "notebooks/1.6_wrapup.html#useful-resources",
    "title": "Day 1 wrap up",
    "section": "Useful resources",
    "text": "Useful resources\n\nnf-core/rnaseq user guide\nFastQC RNAseq example report\nIGV for RNAseq\nRNA-seqlopedia\nCustomising nf-core workshop materials"
  },
  {
    "objectID": "notebooks/2.3_differentialex.html",
    "href": "notebooks/2.3_differentialex.html",
    "title": "Differential expression analysis in R",
    "section": "",
    "text": "Learning objectives\n\n\n\n\nLearn how toperform differential expression analysis with DESeq2 in R.\nDescribe the DeSeq() function.\nVisualise differential expression results in R.\nWe are happy with what we have observed in our exploratory analysis and are finally ready to start differential expression (DE) analysis. We are still at the differential expression analysis stage of our RNAseq experimental workflow:"
  },
  {
    "objectID": "notebooks/2.3_differentialex.html#differential-expression-experimental-design",
    "href": "notebooks/2.3_differentialex.html#differential-expression-experimental-design",
    "title": "Differential expression analysis in R",
    "section": "2.3.1 Differential expression experimental design",
    "text": "2.3.1 Differential expression experimental design\nIn order for DESeq2 to perform DE, we need to revisit the DESeqDataSet object (dds), this time telling it our experimental design. In our case, this will be the column ‚Äúcondition‚Äù, taken from ‚Äúmeta‚Äù.\n\n\n\n\n\n\nMore complex designs with DESeq2\n\n\n\nDesign formulas can be much, much more complex! This gives you the power to model and account for other variation (e.g.¬†you could model batch effects using something like ~ condition + batch)\n\n\ndds <- DESeqDataSetFromMatrix(countData = counttable,\n                              colData = meta,\n                              design = ~ condition)\nLet‚Äôs stop here and take some time to understand dds. In the console, type in the code below:\ndds\nNotice dim - can you tell from this how many genes are are analysing?\ncounts(dds)\nThis extracts the count matrix out of dds.\ncolData(dds)\nThis extracts our experimental design metadata out of dds.\ndesign(dds)\nThis extracts our design formula out of dds."
  },
  {
    "objectID": "notebooks/2.3_differentialex.html#explicitly-set-the-factors-levels",
    "href": "notebooks/2.3_differentialex.html#explicitly-set-the-factors-levels",
    "title": "Differential expression analysis in R",
    "section": "2.3.2 Explicitly set the factors levels",
    "text": "2.3.2 Explicitly set the factors levels\nWhen we perform differential expression and interpret the results, we want to see what changed in the knockout mice (‚Äútreatment‚Äù) compared to the wildtype mice (‚Äúcontrol‚Äù) - not the other way around!\nThe experimental design information in dds is stored as a factor in R (check by running class(dds\\$condition - without the backslash). By default, R will choose a reference level for factors based on alphabetical order. That means, the knockout group is currently our baseline (check by typing in the console: dds$condition, without the backslash).\n\n\n\n\n\n\nMarkdown vs R code writing\n\n\n\nThe backslashes are required to escape the ‚Äú$‚Äù as they are interpreted differently in Markdown vs R.\n\n\nWe will need to explicitly set ‚ÄúWild‚Äù as the baseline level for easier interpretation of results.\n# Set Wild to base level, using relevel\ndds$condition <- relevel(dds$condition, \"Wild\")\n\n# Check that Wild appears as the first level\ndds$condition"
  },
  {
    "objectID": "notebooks/2.3_differentialex.html#perform-differential-expression-analysis",
    "href": "notebooks/2.3_differentialex.html#perform-differential-expression-analysis",
    "title": "Differential expression analysis in R",
    "section": "2.3.3 Perform differential expression analysis",
    "text": "2.3.3 Perform differential expression analysis\nBefore we commence with DE, there are some key concepts that you should know:\n\nEach sample is sequenced to a slightly different depth and this needs to be normalised (if we have 60 million reads for sample A but 30 million for sample B, it does not mean that all of sample A‚Äôs genes are expressed twice as high!)\nRNA-Seq data count data is distributed in a heteroskedatic manner - in other words, the amount of variance changes with the mean. Lowly expressed genes tend to have a higher read count variance than highly expressed genes. This violates the assumption of most statistical models which assume homoskedatic data. Therefore, the data needs to be transformed.\nDifferential expression tests are performed for every single gene. If we use a simple P < 0.05 cut-off value, 1,000 genes will be defined as DE by chance for a species with ~20,000 genes (humans and mice). Therefore, we need to reduce and adjust for multiple testing.\n\nAll DE methods account for the above in their own way. In this workshop, we will use and explore DESeq2‚Äôs method.\n\nVarious Normalization Methods\n\nDESeq2: Size Factor Normalization - Estimates size factors to account for library size and sequencing depth.\nedgeR: Trimmed Mean of M-values (TMM) - Estimates normalization factors based on the most stably expressed genes.\nlimma-voom: Voom Transformation - Transforms count data to log2 counts per million (logCPM) and applies linear modeling.\nCuffdiff/Cufflinks: Quartile Normalization - Scales data based on upper quartile normalization to correct for library size differences."
  },
  {
    "objectID": "notebooks/2.3_differentialex.html#the-deseq-function",
    "href": "notebooks/2.3_differentialex.html#the-deseq-function",
    "title": "Differential expression analysis in R",
    "section": "2.3.4 The DESeq() function",
    "text": "2.3.4 The DESeq() function\nWe are finally ready to perform DE analysis with DESeq2‚Äôs DESeq() function. This performs a number of steps required to perform DE - the console output gives you a clue as to what these steps are doing.\n# Perform DE and store the results back in the dds object\ndds <- DESeq(dds)\n\n# Save the results to res\nres <- results(dds)\nIn brief, by default, DESeq2 is:\n\nEstimating size factors, required to normalise data. DESEq2 uses the median of ratios method. There are many other normalisation methods, each with their pros and cons.\nTransforming the data by estimating dispersion (DESeq2‚Äôs way of quantifying within group variability). DESeq2 uses a negative binomial distribution model.\nPerforming independent filtering to reduce the number of statistical tests to perform. DESeq2 will automatically do this. A common method to do this is by removing lowly expressed genes as these don‚Äôt have enough data confidently test for DE (DESeq2 actually recommends this to also reduce the size and memory required by DESeq())"
  },
  {
    "objectID": "notebooks/2.3_differentialex.html#inspect-the-results",
    "href": "notebooks/2.3_differentialex.html#inspect-the-results",
    "title": "Differential expression analysis in R",
    "section": "2.3.5 Inspect the results",
    "text": "2.3.5 Inspect the results\nGet a summary of results by running the code below:\nsummary(res)\nOrder by the smallest adjusted p value, and have a look at the top 5/bottom 5 DE genes.\nres <- res[order(res$padj), ]\nres\nFrom the above, we can see that DE was performed for KO vs Wild samples for 19,859 genes and 6 columns (6 samples). We then see a table of DE results. The column headers include:\n\nbaseMean: this is an average of the normalized count values, dividing by size factors, taken over all samples. This gives you a general idea of how many reads were detected over all samples present for any one gene.\nlog2FoldChange: This measures the magnitude of differential expression of a gene. A positive value indicates that the KO expression was higher than Wild (remember the fuss about setting factor levels?). This number is on the logarithmic scale to base 2, e.g.¬†log2 fold change of 1.5 means that the gene‚Äôs expression is increased by 2^1.5 = 2.82 times.\nlfcSE: this is the standard error of the log2FoldChange estimate\nstat: Wald statistic\np-value: Wald test p-value\npadj: p-value adjusted for multiple testing. This is sometimes referred to as the false discovery rate or FDR. By default, DESeq2 performs this with the Benjamini Hochberg method. Note - DESeq2 will report ‚ÄúNA‚Äù (not available) values if multiple testing was not applied for this gene, usually because the counts for these gene were too low or the gene was an extreme outlier."
  },
  {
    "objectID": "notebooks/2.3_differentialex.html#define-a-significance-threshold",
    "href": "notebooks/2.3_differentialex.html#define-a-significance-threshold",
    "title": "Differential expression analysis in R",
    "section": "2.3.6 Define a significance threshold",
    "text": "2.3.6 Define a significance threshold\nDifferentially expressed genes are usually defined by cut-offs for two metrics, which are the adjusted p-value and the fold change value. We commonly see differential expression defined as genes with:\n\nAdjusted p-value of < 0.05 (sometimes < 0.1)\nFold change of 2 (log2 fold change ~ 1)\n\nThis is somewhat arbitrary - we need to have just the right number of differentially expressed genes to perform pathway analysis (around 100 - 3,000 is a general guide). Gene expression should be thought of in a biological context - we care about the ‚Äútop‚Äù most differentially expressed genes."
  },
  {
    "objectID": "notebooks/2.3_differentialex.html#subset-the-data-and-write-out-results",
    "href": "notebooks/2.3_differentialex.html#subset-the-data-and-write-out-results",
    "title": "Differential expression analysis in R",
    "section": "2.3.7 Subset the data and write out results",
    "text": "2.3.7 Subset the data and write out results\nHere we will use padj < 0.05 as our cut-off value for significance and use these genes for enrichment analysis.\n# Redefine the significance cut-off used for independent filtering (default = 0.1). \n# This should be done if we want to use p adj to a value other than 0.1 \nres_padj0.05 <- results(dds, alpha = 0.05)\n\n# Subset the results and write these to an output file\nresSig005_subset<-subset(res_padj0.05, padj < 0.05)\nwrite.table(resSig005_subset, \n            \"res_DeSeq2_FDR0.05_comparison_Wild_vs_KO_FUllMatrix.tab\", \n            sep=\"\\t\", \n            col.names=NA, \n            quote=F)\n\n# Reformat the output results into a data.frame\nresSig005_subset <- data.frame(genes = row.names(resSig005_subset), resSig005_subset)\n\n# We can also order padj-filtered results by log fold change\nresSig005_subset_lfc <- resSig005_subset[order(resSig005_subset$log2FoldChange), ]\n\n# Notice how our summary of results has changed slightly now\nsummary(res_padj0.05)\nNormalised count data can be used for visualisation/other analyses. The code below extracts and prints normalised counts to file.\n# Extract normalized counts from dds\nnormalised_counts <- counts(dds, normalized = TRUE)\n\n# Save normalized counts (tab separated) to file\nwrite.table(normalised_counts, \n            \"normalised_all_samples_DeSeq2_FUllMatrix.tab\", \n            sep = \"\\t\", \n            col.names = NA, \n            quote = F)\nNormalised count data can be used for visualisation/other analyses, so it is also handy to save these results.\nnormalised_counts <- counts(dds,normalized = TRUE)\nwrite.table(normalised_counts, \"normalised_all_samples_DeSeq2_FUllMatrix.tab\", sep = \"\\t\", col.names = NA, quote = F)"
  },
  {
    "objectID": "notebooks/2.3_differentialex.html#visualise-the-results",
    "href": "notebooks/2.3_differentialex.html#visualise-the-results",
    "title": "Differential expression analysis in R",
    "section": "2.3.8 Visualise the results",
    "text": "2.3.8 Visualise the results\n\nVolcano plot\nThe volcano plot is a scatterplot that shows magnitude of change (fold change, x axis) against statistical significance (p-value, y axis). It provides an overall visual snapshot of the number of up and downregulated genes that are statistically significant.\n# Create a basic volcano plot (scatter plot) with x axis = LogFC, y axis = -log10(pvalue)\nresdata <- as.data.frame(res)\n\n# Define whether genes are significantly DE or not and store this in a new column called DE\nresdata$Significant <- \"No\"\nresdata$Significant[resdata$log2FoldChange > 1 & resdata$pvalue < 0.05 ] <- \"Upregulated\"\nresdata$Significant[resdata$log2FoldChange < -1 & resdata$pvalue < 0.05 ] <- \"Downregulated\"\n\n# Create the volcano plot\np <- ggplot(data=resdata,\n       aes(x=log2FoldChange, y=-log10(pvalue), col=Significant)) + geom_point()\n\n# Add significance lines at log2FoldChange -1, 1 and pvalue 0.05\np2 <- p + geom_vline(xintercept = c(-1, 1), col = \"red\") +\n    geom_hline(yintercept = -log10(0.05), col = \"red\")\n\n# Print the plot\np2\n\n\nVisualise some DE genes\nWe have applied low read-count filtering followed by appropriate statistical tests using the DESeq2 package for identification of the differentially expressed geens across our conditions of interest.\nHowever we recommend that you visualise a few genes (of specific interest or otherwise) to check if the identification of these genes is supported by sufficient read-counts.\nUse plotCounts function to plot normalized counts for a single gene of interest. Here we plot\nplotCounts(dds, \n           gene = \"Dip2b\", \n           intgroup = \"condition\")\n\n\n\n\n\n\nChallenge exercise\n\n\n\nChoose a significant gene that is downregulated in the knockout mice. Enter the plotCounts code in the grey box below to plot the normalized counts for each sample for the gene you have chosen.\n\n\n\n\n\n\n\n\nReflection exercise\n\n\n\nWhy should we observe the expressions of DE genes before shortlisting them for experimental validation?\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nBig fold changes (and high significance - indicated by low Padj) can be of importance\nBorderline significance can be due to high variance b/w replicates\nSometime one of the samples in a small cohort can be an outlier\n\n\n\n\n\n\nDiagnostic plots\nBefore we get too excited about our results, we need to confirm that DESeq2‚Äôs assumptions were met and that statistical analysis was performed appropriately. We will explore a few plots and concepts to better understand what is happening under the hood.\n\nMA plot\nThe MA plot provides an overview of the relationship between significantly differentially expressed genes, gene expression and log fold change in the form of a scatter plot. Each dot on the plot represents a single gene, significant genes are coloured as a blue dot. The average gene expression is on the x axis (expressed as a mean of normalized counts) and the log fold change is on the Y axis.\n# There is another R function called plotMA, so we need to specify to use DESeq2's plotMA\nDESeq2::plotMA(res, ylim = c(-12, 12))\nThere are a few things we notice:\n\nGenes that are differentially expressed will appear as points deviating significantly from the centerline, with those above the line indicating upregulated genes and those below indicating downregulated genes.\nGenes with a lower mean expression have higher variable log fold changes (heteroskedatic - as we expected)\nGene expression is symmetrical around log fold change 0\n\n\n\nHistogram of P-values\nRemember that for every gene, we perform a statistical test to determine whether gene expression is significantly different in the knockout samples, compared to the wildtype. This results in thousands (~20,000 genes in the mouse genome) of p-values. We can look at the histogram of p-values to see how our well our statistical test behaves before we apply correction for multiple testing.\n# Bin frequency of p-value counts by 0.05 incremets (i.e plot 20 columns from p-value of 0 to 1)\nhist(res$pvalue, \n     breaks = 20, \n     col = \"grey\")\nA nice histogram of p-values will have a peak at the 0.05 end, and a uniform frequency at all other p-value bins. Think back to your null and alternate hypothesis. Under the null hypothesis, there is a 5% chance of genes will fall under p-value 0.05, 10 % for p-value under 0.1, etc. The high peak at the first bin (p-value 0 - 0.5) represents genes that reject the null hypothesis (in addition to all the false discoveries - hence our need to adjust for multiple testing!).\nA histogram of p-values that looks anything other than what is described above means that something weird has happened and you may need to contact your local statistician/bioinformatician.\nThis blog post has a nice explanation of each scenario if you want to explore this further.\n\n\n\nAdditional plots\n\nDispersion estimates\nThe dispersion plot is useful to examine whether your data is meeting DESeq2‚Äôs assumptions around heteroskedasticity and that the data fits DESeq2‚Äôs model well. Dispersion is how DESeq2 quantifies variability in the data. It considers variance and mean expression within each experimental group.\nLet‚Äôs use plotDispEsts() to generate the dispersion plot and discuss what this means.\n# Plot dispersion estimates using dds\n# Note - we have set our experimental design to ~ condition and it is using this to estimate dispersion\nplotDispEsts( dds)\nThere are a few things to note:\n\nDispersion is higher for genes with small mean of normalized counts, and lower for genes with high mean of normalized counts.\n\nIf you see any other trend, this is a sign that you should not trust DESeq2‚Äôs results and that you need to investigate further\n\nTo transform the data, we need to use the variation observed within each experimental group. We cannot do this accurately with few biological replicates (e.g n =3 for KO, n = 3 for wildtype).\n\nDESeq2 assumes that genes with similar expression have a similar level of dispersion to get a more accurate estimation of variability - one of its many benefits! A model (the red curve) is calculated by DESeq2 with this information, using a method called shrinkage.\nIn other words, the red line represents the expected dispersion for any given level of expression\n\nThe black dots represent each gene and their own dispersion (using within group variance as described above)\nThe gene-wise dispersion estimate (black dots) need to be shrunken towards the red line. This helps to reduce false positive results in our differential expression analysis\n\nThere is a lot happening here, but the main point is that our dispersion plot looks as expected and plots should generally appear like this. Check this website for a deeper explanation of this concept, and for examples of what bad dispersion plots look like."
  },
  {
    "objectID": "notebooks/2.3_differentialex.html#experimental-validation",
    "href": "notebooks/2.3_differentialex.html#experimental-validation",
    "title": "Differential expression analysis in R",
    "section": "2.3.9 Experimental validation",
    "text": "2.3.9 Experimental validation\nIt is generally a good practice to validate differentially expressed genes (DEGs) experimentally after identifying them through high-throughput techniques such as RNAseq. Validating DEGs experimentally helps confirm the reliability of the computational analysis and provides biological context to the results. qPCR is a widely used technique to validate gene expression changes. It allows for the accurate measurement of the expression levels of specific genes and can confirm whether the observed differences in gene expression are consistent with the computational analysis.\n\n\n\n\n\n\nKey takeaways\n\n\n\n\nWe need to normalise our DE data to account for differences in sequencing depth between samples.\nWe need to transform our data to make sure we don‚Äôt violate the assumptions of statistical models we‚Äôre applying.\nWe need to account for multiple testing, as we are performing a DE test for every gene.\n\nGene expression should be thought of in its biological context, consider this when defining your significance threshold.\nVisualising our results can aid our interpretation and confirm the statistical assumptions have been met.\n\nThe object class used by the DESeq2 package to store the read counts and the intermediate estimated quantities during statistical analysis is the DESeqDataSet."
  },
  {
    "objectID": "tipstricks.html",
    "href": "tipstricks.html",
    "title": "Tips and tricks",
    "section": "",
    "text": "nf-core/rnaseq user guide\nnf-core tools utility\nnf-core download instructions\nnf-core institutional configs\nnf-core Slack channel"
  },
  {
    "objectID": "tipstricks.html#nf-corernaseq",
    "href": "tipstricks.html#nf-corernaseq",
    "title": "Tips and tricks",
    "section": "",
    "text": "nf-core/rnaseq user guide\nnf-core tools utility\nnf-core download instructions\nnf-core institutional configs\nnf-core Slack channel"
  },
  {
    "objectID": "tipstricks.html#r-container-and-notebook",
    "href": "tipstricks.html#r-container-and-notebook",
    "title": "Tips and tricks",
    "section": "R container and notebook",
    "text": "R container and notebook\nThis workshop is designed to be portable and reproducible. We have provided the Rmd file and RStudio/4.0.1 container for you to use:\n\nRNA differential expression notebook\nRStudio/4.0.1 for RNAseq container"
  },
  {
    "objectID": "tipstricks.html#additional-training",
    "href": "tipstricks.html#additional-training",
    "title": "Tips and tricks",
    "section": "Additional training",
    "text": "Additional training\n\nnf-core training calendar\nnf-core tutorials\nCustomising nf-core workshop\nIntro to the shell exercises\nR for reproducible science workshop"
  },
  {
    "objectID": "tipstricks.html#troubleshooting",
    "href": "tipstricks.html#troubleshooting",
    "title": "Tips and tricks",
    "section": "Troubleshooting",
    "text": "Troubleshooting\n\nError when trying to run rserver on Nimbus\nWhen you run the command\nmkdir -p /tmp/rstudio-server  \n\nPASSWORD='abc' singularity exec \\  \n    -B /tmp/rstudio-server:/var/lib/rstudio-server \\  \n    -B /tmp/rstudio-server:/var/run/rstudio-server \\  \n    -B /home/ubuntu/working_directory/Day-2:/home \\   \n    rstudio_4.1.0.sif \\  \n    rserver --auth-none=0 --auth-pam-helper-path=pam-helper --server-user ubuntu  \nIf an error similar to the one below pops up -\n06 Oct 2023 01:16:34 [rserver] ERROR system error 98 (Address already in use); OCCURRED AT rstudio::core::Error rstudio::core::http::initTcpIpAcceptor(rstudio::core::http::SocketAcceptorService<rstudio_boost::asio::ip::tcp>&, const string&, const string&) src/cpp/server/ServerInit.cpp:103; LOGGED FROM: int main(int, char* const*) src/cpp/server/ServerMain.cpp:704\nDo the following\nsudo lsof -i :8787\nIf you see:\nCOMMAND PID           USER   FD   TYPE DEVICE SIZE/OFF NODE NAME\nrserver 747 rstudio-server    7u  IPv4  19251      0t0  TCP *:8787 (LISTEN)\nRun the command:\nsudo kill <PID> \nHere substitute  with the corresponding PID value e.g.¬†747 above\n\n\nRStudio not loading in browser\nIf you need to end your session, exit your browser and on Nimbus, run the following command:\nlsof -ti:8787 | xargs kill -9\n\n\nLoop device error on Nimbus\nWhen running Singularity containers on Nimbus you may come across an error:\nFATAL:   container creation failed: mount /proc/self/fd/3->/usr/local/var/singularity/mnt/session/rootfs \nerror: while mounting image /proc/self/fd/3: failed to find loop device: could not attach image file to loop device: no loop devices available\nRun the following commands to resolve this issue:\nlsmod | grep loop\nsudo modprobe loop\nsudo losetup -f\n/dev/loop9"
  },
  {
    "objectID": "notebooks/2.5_wrapup.html#experimental-results",
    "href": "notebooks/2.5_wrapup.html#experimental-results",
    "title": "Day 2 wrap up",
    "section": "Experimental results",
    "text": "Experimental results\n\n2.5.1 Exploratory analysis\nThe principle component analysis shows a good separation of the sample across conditions:\n\n\n\n2.5.2 Differential expression analysis\nGtf2ird1 KO showed dysregulation of many genes and many functionally enriched gene ontologies (GO).\nTotal 19,859 genes in mouse genome.\n- LFC &gt; 0 (up): 1,353 DE genes (6.8%)\n- LFC &lt; 0 (down): 984 DE genes (5%)\n\n\n\n2.5.3 Functional enrichment analysis\nTotal GO enrichments identified from DE genes.\n- Categories form Up-regulated genes : 823\n- Categories form Down-regulated genes : 255\n\nThe most significantly enriched GO terms captured high-level biological functions that weren‚Äôt all related to our phenotypes of interest. We will go back to the biology to make sense of all these results.\nWhich ontologies in the list are relevant to your experiment\nCraniofacial development\nDisorders relate to the bones of the skull (cranio) and face. Lead to distinctive facial features‚Äã\nGO: Wnt pathway\nInvolved in\n- cytoskeletal dynamics and cell adhesion\n- promote the differentiation of skin epithelial cells and the development of hair follicles\nGO: skin development, epidermis development\nCardiovascular abnormalities\nGO: striated muscle tissue development\nGO: muscle system process, muscle cell development, muscle cell differentiation\nGO: sarcomere organisation\n\n\n2.5.4 Experimental design considerations\n\nExperimental design: Choose the right tissue for the research question.\n\nNumber of replicates: To improve statistical power and overcome any problems due to outliers.\n\nRealistic interpretation of the results.\n\n\n\n2.5.5 Reproducible analysis using Rstudio with Rmarkdown\nWe used Singularity containers for portable and reproducible analysis"
  },
  {
    "objectID": "notebooks/2.5_wrapup.html#exploratory-analysis",
    "href": "notebooks/2.5_wrapup.html#exploratory-analysis",
    "title": "Day 2 wrap up",
    "section": "2.5.1 Exploratory analysis",
    "text": "2.5.1 Exploratory analysis\nThe principle component analysis shows a good separation of the sample across conditions on PC2 which accounts for the majority (80%) of variance. A clear separation of samples across conditions indicates differences in gene expression profiles between our samples are substantial and suggests the biological conditions we are comparing have a strong effect at the transcriptome level."
  },
  {
    "objectID": "notebooks/2.5_wrapup.html#differential-expression-analysis",
    "href": "notebooks/2.5_wrapup.html#differential-expression-analysis",
    "title": "Day 2 wrap up",
    "section": "2.5.2 Differential expression analysis",
    "text": "2.5.2 Differential expression analysis\nThe Gtf2ird1 KO mice showed dysregulation of many genes. Out of the 19,859 genes in the mm10 mouse genome:\n\n1,353 DE genes (6.8%) with a log-fold change (LFC) >0 were upregulated\n984 DE genes (5%) with a LFC <0 were downregulated\n\nThis suggests that Gtf2ird1 plays a regulatory role on many genes in the mouse genome. The elevated number of upregulated genes may suggest that Gtf2ird1 function acts as a repressor for many genes or is involved in pathways that have a net repressive effect. These changes may result in observable phenotypic effects."
  },
  {
    "objectID": "notebooks/2.5_wrapup.html#functional-enrichment-analysis",
    "href": "notebooks/2.5_wrapup.html#functional-enrichment-analysis",
    "title": "Day 2 wrap up",
    "section": "2.5.3 Functional enrichment analysis",
    "text": "2.5.3 Functional enrichment analysis\nFunctional enrichment analysis revealed enrichment of multiple Gene Ontology (GO) categories:\n\nGO categories enriched in up-regulated genes: 823\n\nGO categories enriched in down-regulated genes: 255\n\n\nThe most significantly enriched GO terms captured high-level biological functions that were not directly related to our phenotypes of interest. To make sense of these results, we need to examine the biological assumptions of our experiment. We identified a number of relevant GO terms relevant to this case study:\n\nCraniofacial development and distinctive facial features‚Äã in WBS patients\n\nGO:0016055 Wnt signaling pathway (involved in cytoskeletal dynamics, cell adhesion, differentiation of skin epithelial cells and the development of hair follicles).\n\nGO:0043588 skin development.\nGO:0008544 epidermis development.\n\nCardiovascular abnormalities in WBS patients\n\nGO:0014706 striated muscle tissue development.\n\nGO:0003012 muscle system process,\nGO:0055001 muscle cell development\nGO:0042692 muscle cell differentiation.\n\nGO:0045214 sarcomere organisation."
  },
  {
    "objectID": "notebooks/2.5_wrapup.html#experimental-considerations-limitations.",
    "href": "notebooks/2.5_wrapup.html#experimental-considerations-limitations.",
    "title": "Day 2 wrap up",
    "section": "2.5.4 Experimental considerations / limitations.",
    "text": "2.5.4 Experimental considerations / limitations.\n\nExperimental design: Choose the right tissue for the research question.\n\nNumber of replicates: To improve statistical power and overcome any problems due to outliers.\n\nRealistic interpretation of the results."
  },
  {
    "objectID": "notebooks/2.5_wrapup.html#experimental-design-limitations",
    "href": "notebooks/2.5_wrapup.html#experimental-design-limitations",
    "title": "Day 2 wrap up",
    "section": "2.5.4 Experimental design limitations",
    "text": "2.5.4 Experimental design limitations\nWe were unable to reliably comment on phenotypic changes relating to cognitive traits observed in patients with WBS. There were some other potentially limiting factors in our case study‚Äôs experimental design including:\n\nThe choice of tissue: The lip epidermal tissue cannot reflect cognitive and neurological traits and is thus is a big limitation.\nThe number of replicates: To improve statistical power and overcome any problems due to outliers, it is necessary to include more replicates per condition."
  },
  {
    "objectID": "notebooks/1.6_wrapup.html#key-takeaways-for-day-1",
    "href": "notebooks/1.6_wrapup.html#key-takeaways-for-day-1",
    "title": "Day 1 wrap up",
    "section": "Key takeaways for day 1",
    "text": "Key takeaways for day 1\n\nRaw data quality control is an essential step, it allows you to identify any potential issues that may interfere with analyses.\nThe nf-core/rnaseq pipeline follows community best practices and offers a reproducible, portable, and user friendly method for pre-processing your RNAseq data.\nFastQC is a useful tool for fastq quality inspection, however it was not built for RNAseq data so your RNAseq data will always fail some of their tests.\nRead trimming is not always necessary. Your choice to trim reads will depend on their quality, presence of adapter sequences, and read alignment tool of choice. Over-trimming or unnecessary trimming can sometimes remove valuable sequence information, so it‚Äôs essential to strike a balance.\nRNAseq read alignment is splice-aware, meaning that it‚Äôs designed to identify and handle reads that span exon-exon junctions. This capability ensures that such reads are accurately mapped, providing a true representation of the transcriptome.\nRead quantification to create a count matrix for your samples is required for differential expression analysis. Proper quantification will ensure that RNA abundance is accurately represented.\nDifferent alignment and quantification tools and methods suit different applications. These tools will have different underlying models and assumptions that will be reflected in their outputs."
  },
  {
    "objectID": "notebooks/2.6_workshop_summary.html#day-2-counts-to-genes-and-functional-enrichments",
    "href": "notebooks/2.6_workshop_summary.html#day-2-counts-to-genes-and-functional-enrichments",
    "title": "Workshop summary",
    "section": "2.6.2 Day 2: Counts to genes and functional enrichments",
    "text": "2.6.2 Day 2: Counts to genes and functional enrichments\n\nOn Day 2 we used the raw counts to identify the differentially expressed genes and highlighted the relevant functions.\nThe following steps were identified to be essential:\n\nPerform an exploratory analysis of the count data for quality control.\nAnalyse the count data to identify differentially expressed genes.\n\nIdentify functional enrichments from differentially expressed genes.\n\nWe discussed how to perform reproducible analysis in Rstudio with Rmarkdown using singularity containers."
  },
  {
    "objectID": "notebooks/2.5_wrapup.html#useful-resources",
    "href": "notebooks/2.5_wrapup.html#useful-resources",
    "title": "Day 2 wrap up",
    "section": "2.5.5 Useful resources",
    "text": "2.5.5 Useful resources\n\nR\nRStudio\nGO enrichment analysis"
  },
  {
    "objectID": "notebooks/2.6_workshop_summary.html#key-takeaways-for-day-1",
    "href": "notebooks/2.6_workshop_summary.html#key-takeaways-for-day-1",
    "title": "Workshop summary",
    "section": "Key takeaways for day 1",
    "text": "Key takeaways for day 1\n\nRaw data quality control is an essential step, it allows you to identify any potential issues that may interfere with analyses."
  },
  {
    "objectID": "notebooks/2.6_workshop_summary.html#day-1-raw-sequence-to-gene-counts-1",
    "href": "notebooks/2.6_workshop_summary.html#day-1-raw-sequence-to-gene-counts-1",
    "title": "Workshop summary",
    "section": "2.6.1 Day 1: Raw sequence to gene counts",
    "text": "2.6.1 Day 1: Raw sequence to gene counts\nOn Day 1 we used the nf-core/rnaseq pipeline to convert raw RNAseq data into raw counts.\n\nWe discussed the following essential steps in a RNAseq analysis workflow:\n\nCheck the quality of raw-reads.\nTrim the raw-reads to get rid of bad-quality read-regions and/or bad-quality reads.\n\nAlign the trimmed-reads to reference sequence to identify where they belong\n\nQuantify the aligned reads to get gene-level read-counts.\n\nWe discussed the workflow management tool nextflow which can be used for automated, reproducible, flexible and portable analysis.\n\nWe excecuted the nf-core/rnaseq pipeline and interpreted its outputs."
  },
  {
    "objectID": "notebooks/2.6_workshop_summary.html#day-2-counts-to-genes-and-functional-enrichments-1",
    "href": "notebooks/2.6_workshop_summary.html#day-2-counts-to-genes-and-functional-enrichments-1",
    "title": "Workshop summary",
    "section": "2.6.2 Day 2: Counts to genes and functional enrichments",
    "text": "2.6.2 Day 2: Counts to genes and functional enrichments\n\nOn Day 2 we used the raw counts to identify the differentially expressed genes and highlighted the relevant functions.\nThe following steps were identified to be essential:\n\nPerform an exploratory analysis of the count data for quality control.\nAnalyse the count data to identify differentially expressed genes.\n\nIdentify functional enrichments from differentially expressed genes.\n\nWe also discussed how to perform reproducible analysis using Rstudio with Rmarkdown using singularity containers."
  },
  {
    "objectID": "notebooks/2.6_workshop_summary.html#day-1-raw-sequence-to-gene-counts",
    "href": "notebooks/2.6_workshop_summary.html#day-1-raw-sequence-to-gene-counts",
    "title": "Workshop summary",
    "section": "",
    "text": "On Day 1 we used the nf-core/rnaseq pipeline to convert raw RNAseq data into raw counts.\nWe discussed the following essential steps in a RNAseq analysis workflow:\n\nCheck the quality of raw-reads.\nTrim the raw-reads to get rid of bad-quality read-regions and/or bad-quality reads.\n\nAlign the trimmed-reads to reference sequence to identify where they belong\n\nQuantify the aligned reads to get gene-level read-counts.\n\nNext:\n\nWe discussed the workflow management tool nextflow which is used for automated, reproducible, flexible and portable analysis.\n\nWe excecuted the nf-core/rnaseq pipeline and interpreted its outputs."
  }
]